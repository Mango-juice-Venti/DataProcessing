{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac27b95",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "4c38ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c2b49",
   "metadata": {},
   "source": [
    "# Read csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "b26e1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../../data/sample_submission.csv')\n",
    "holidays = pd.read_csv('../../data/holidays_2023_2025.csv')\n",
    "train = pd.read_csv('../../data/train/train.csv')\n",
    "# os.getcwd()\n",
    "terms = pd.read_csv('../solar_term_2023_2025.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa6c8f1",
   "metadata": {},
   "source": [
    "# Feature about date\n",
    "1. year\n",
    "2. month\n",
    "3. day\n",
    "4. weekday\n",
    "5. is_holiday\n",
    "6. is_sandwich\n",
    "7. is_before_holiday\n",
    "8. is_after_holiday\n",
    "9. is_weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "780e82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 날짜 범위\n",
    "dates = pd.date_range(start=\"2023-01-01\", end=\"2025-05-31\", freq=\"D\")\n",
    "\n",
    "# 연, 월, 일 col 생성\n",
    "train['영업일자'] = pd.to_datetime(train['영업일자'])\n",
    "train['time_idx'] = (train['영업일자'] - train['영업일자'].min()).dt.days\n",
    "train['year'] = train['영업일자'].dt.year\n",
    "train['month'] = train['영업일자'].dt.month\n",
    "train['day'] = train['영업일자'].dt.day\n",
    "# 영업장, 메뉴명 col 생성\n",
    "train[['영업장명', '메뉴명']] = train['영업장명_메뉴명'].str.split('_', expand=True)\n",
    "# 요일 col 생성\n",
    "train['weekday'] = train['영업일자'].dt.weekday.astype(int)\n",
    "\n",
    "# 공휴일 col 생성\n",
    "holidays[\"locdate\"] = pd.to_datetime(holidays[\"locdate\"])\n",
    "holiday_dates = set(holidays['locdate'])\n",
    "train['is_holiday'] = train['영업일자'].isin(holiday_dates).astype(int)\n",
    "\n",
    "train['is_weekend'] = train['weekday'].apply(lambda x: 1 if x in [5, 6] else 0)\n",
    "# 5) isSandwich: 오늘은 평일(0)이고, 어제/내일이 모두 쉬는 날(1)인 경우 1\n",
    "train[\"non_work\"] = ((train[\"is_holiday\"] == 1) | (train[\"is_weekend\"] == 1)).astype(int)\n",
    "\n",
    "train[\"is_sandwich\"] = 0\n",
    "train.loc[\n",
    "    (train[\"non_work\"] == 0) &\n",
    "    (train[\"non_work\"].shift(1) == 1) &\n",
    "    (train[\"non_work\"].shift(-1) == 1),\n",
    "    \"is_sandwich\"\n",
    "] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "cc306583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 요일 컬럼 만들기\n",
    "train['weekday'] = pd.to_datetime(train['영업일자']).dt.weekday  # 0=월, 6=일\n",
    "\n",
    "# 2) 메뉴별 요일별 평균 매출 계산\n",
    "menu_dow_avg = (\n",
    "    train.groupby(['영업장명_메뉴명','weekday'], as_index=False)['매출수량']\n",
    "         .mean()\n",
    "         .rename(columns={'매출수량':'요일평균매출'})\n",
    ")\n",
    "\n",
    "# 3) 메뉴별 min–max 스케일링 (요일 단위)\n",
    "g = menu_dow_avg.groupby('영업장명_메뉴명')['요일평균매출']\n",
    "menu_dow_avg['weekday_score'] = (menu_dow_avg['요일평균매출'] - g.transform('min')) / \\\n",
    "                             (g.transform('max') - g.transform('min'))\n",
    "\n",
    "# max=min → 모든 요일 평균이 동일 → 1로 통일\n",
    "menu_dow_avg.loc[g.transform('max') == g.transform('min'), 'weekday_score'] = 1.0\n",
    "\n",
    "# 4) 원본 데이터에 merge\n",
    "train = train.merge(\n",
    "    menu_dow_avg[['영업장명_메뉴명','weekday','weekday_score']],\n",
    "    on=['영업장명_메뉴명','weekday'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "4851e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0/1로 되어 있다고 가정: non_work = (주말 or 휴일) 1, 평일 0\n",
    "train['영업일자'] = pd.to_datetime(train['영업일자'])\n",
    "\n",
    "# 1) 날짜 단위 non_work(하루에 여러 행이면 max로 대표)\n",
    "daily = (train.groupby('영업일자', as_index=False)['non_work']\n",
    "               .max()\n",
    "               .sort_values('영업일자'))\n",
    "\n",
    "# 2) 날짜 기준으로 before/after 플래그 계산\n",
    "daily['is_before_holiday'] = (\n",
    "    (daily['non_work'].eq(0)) &\n",
    "    (daily['non_work'].shift(-1, fill_value=0).eq(1))\n",
    ").astype(int)\n",
    "\n",
    "daily['is_after_holiday'] = (\n",
    "    (daily['non_work'].eq(0)) &\n",
    "    (daily['non_work'].shift(1, fill_value=0).eq(1))\n",
    ").astype(int)\n",
    "\n",
    "# 3) 원본 train에 날짜로 merge\n",
    "train = train.merge(\n",
    "    daily[['영업일자', 'is_before_holiday', 'is_after_holiday']],\n",
    "    on='영업일자', how='left'\n",
    ")\n",
    "\n",
    "# 필요하면 non_work 컬럼 정리\n",
    "# train = train.drop(columns=['non_work'], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b0a37",
   "metadata": {},
   "source": [
    "# Feature about top menu --todo--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "ae30ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시: train 데이터에 영업장명, 메뉴명, 매출수량 컬럼이 있다고 가정\n",
    "# train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# 1) 영업장별 메뉴별 총매출\n",
    "menu_sales = (\n",
    "    train.groupby(['영업장명', '메뉴명'], as_index=False)['매출수량']\n",
    "         .sum()\n",
    "         .rename(columns={'매출수량': '총매출수량'})\n",
    ")\n",
    "\n",
    "# 2) 영업장 안에서 순위(내림차순: 많이 팔린 메뉴가 1위)\n",
    "menu_sales['rank'] = menu_sales.groupby('영업장명')['총매출수량'] \\\n",
    "                               .rank(method='average', ascending=False)\n",
    "\n",
    "# 3) 영업장별 메뉴 개수\n",
    "menu_sales['n'] = menu_sales.groupby('영업장명')['총매출수량'].transform('size')\n",
    "\n",
    "# 4) 0~1 스케일: (n - rank) / (n - 1)\n",
    "#    → rank=1이면 1, rank=n이면 0 (n=1인 경우 예외 처리)\n",
    "menu_sales['rank01'] = (menu_sales['n'] - menu_sales['rank']) / (menu_sales['n'] - 1)\n",
    "menu_sales.loc[menu_sales['n'] == 1, 'rank01'] = 1.0  # 해당 영업장에 메뉴가 1개뿐이면 1\n",
    "\n",
    "# 5) 원본 train에 붙이기\n",
    "train = train.merge(\n",
    "    menu_sales[['영업장명', '메뉴명', 'rank01']], \n",
    "    on=['영업장명', '메뉴명'], how='left'\n",
    ").rename(columns={'rank01': 'menu_rank'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea23ce",
   "metadata": {},
   "source": [
    "# menu_ category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "5a8b884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_category = {\n",
    "    '1인 수저세트': '기타',\n",
    "    'BBQ55(단체)': '메인메뉴',\n",
    "    '대여료 60,000원': '기타',\n",
    "    '대여료 30,000원': '기타',\n",
    "       '대여료 90,000원': '기타',\n",
    "       '본삼겹 (단품,실내)': '메인메뉴',\n",
    "       '스프라이트 (단체)': '음료',\n",
    "       '신라면': '추가메뉴',\n",
    "       '쌈야채세트': '추가메뉴',\n",
    "       '쌈장': '추가메뉴',\n",
    "       '육개장 사발면': '추가메뉴',\n",
    "       '일회용 소주컵': '기타',\n",
    "       '일회용 종이컵': '기타',\n",
    "       '잔디그늘집 대여료 (12인석)': '기타',\n",
    "       '잔디그늘집 대여료 (6인석)': '기타',\n",
    "       '잔디그늘집 의자 추가': '기타',\n",
    "       '참이슬 (단체)': '주류',\n",
    "       '친환경 접시 14cm': '기타',\n",
    "       '친환경 접시 23cm': '기타',\n",
    "       '카스 병(단체)': '주류',\n",
    "       '콜라 (단체)': '음료',\n",
    "       '햇반': '추가메뉴',\n",
    "       '허브솔트': '추가메뉴',\n",
    "       '(단체) 공깃밥': '추가메뉴',\n",
    "       '(단체) 생목살 김치전골 2.0': '메인메뉴',\n",
    "       '(단체) 은이버섯 갈비탕': '메인메뉴',\n",
    "       '(단체) 한우 우거지 국밥': '메인메뉴',\n",
    "       '(단체) 황태해장국 3/27까지': '메인메뉴',\n",
    "       '(정식) 된장찌개': '메인메뉴',\n",
    "       '(정식) 물냉면 ': '메인메뉴',\n",
    "       '(정식) 비빔냉면': '메인메뉴',\n",
    "       '(후식) 된장찌개': '추가메뉴',\n",
    "       '(후식) 물냉면': '추가메뉴',\n",
    "       '(후식) 비빔냉면': '추가메뉴',\n",
    "       '갑오징어 비빔밥': '메인메뉴',\n",
    "       '갱시기': '메인메뉴',\n",
    "       '공깃밥': '추가메뉴',\n",
    "       '꼬막 비빔밥': '메인메뉴',\n",
    "       '느린마을 막걸리': '주류',\n",
    "       '담하 한우 불고기': '메인메뉴',\n",
    "       '담하 한우 불고기 정식': '메인메뉴',\n",
    "       '더덕 한우 지짐': '메인메뉴',\n",
    "       '들깨 양지탕': '메인메뉴',\n",
    "       '라면사리': '추가메뉴',\n",
    "       '룸 이용료': '기타',\n",
    "       '메밀면 사리': '추가메뉴',\n",
    "       '명인안동소주': '주류',\n",
    "       '명태회 비빔냉면': '메인메뉴',\n",
    "       '문막 복분자 칵테일': '주류',\n",
    "       '봉평메밀 물냉면': '메인메뉴',\n",
    "       '생목살 김치찌개': '메인메뉴',\n",
    "       '스프라이트': '음료',\n",
    "       '은이버섯 갈비탕': '메인메뉴',\n",
    "       '제로콜라': '음료',\n",
    "       '참이슬': '주류',\n",
    "       '처음처럼': '주류',\n",
    "       '카스': '주류',\n",
    "       '콜라': '음료',\n",
    "       '테라': '주류',\n",
    "       '하동 매실 칵테일': '주류',\n",
    "       '한우 떡갈비 정식': '메인메뉴',\n",
    "       '한우 미역국 정식': '메인메뉴',\n",
    "       '한우 우거지 국밥': '메인메뉴',\n",
    "       '한우 차돌박이 된장찌개': '메인메뉴',\n",
    "       '황태해장국': '메인메뉴',\n",
    "       'AUS (200g)': '메인메뉴',\n",
    "       'G-Charge(3)': '기타',\n",
    "       'Gls.Sileni': '주류',\n",
    "       'Gls.미션 서드': '주류',\n",
    "       'Open Food': '기타',\n",
    "       '그릴드 비프 샐러드': '메인메뉴',\n",
    "       '까르보나라': '메인메뉴',\n",
    "       '모둠 해산물 플래터': '메인메뉴',\n",
    "       '미션 서드 카베르네 쉬라': '메인메뉴',\n",
    "       '버섯 크림 리조또': '메인메뉴',\n",
    "       '빵 추가 (1인)': '추가메뉴',\n",
    "       '시저 샐러드 ': '메인메뉴',\n",
    "       '아메리카노': '음료',\n",
    "       '알리오 에 올리오 ': '메인메뉴',\n",
    "       '양갈비 (4ps)': '메인메뉴',\n",
    "       '자몽리치에이드': '음료',\n",
    "       '하이네켄(생)': '주류',\n",
    "       '한우 (200g)': '메인메뉴',\n",
    "       '해산물 토마토 리조또': '메인메뉴',\n",
    "       '해산물 토마토 스튜 파스타': '메인메뉴',\n",
    "       '해산물 토마토 스파게티': '메인메뉴',\n",
    "       '(단체)브런치주중 36,000': '메인메뉴',\n",
    "       '(오븐) 하와이안 쉬림프 피자': '메인메뉴',\n",
    "       '(화덕) 불고기 페퍼로니 반반피자': '메인메뉴',\n",
    "       'BBQ Platter': '메인메뉴',\n",
    "       'BBQ 고기추가': '추가메뉴',\n",
    "       '글라스와인 (레드)': '주류',\n",
    "       '레인보우칵테일(알코올)': '주류',\n",
    "       '미라시아 브런치 (패키지)': '메인메뉴',\n",
    "       '버드와이저(무제한)': '주류',\n",
    "       '보일링 랍스타 플래터': '메인메뉴',\n",
    "       '보일링 랍스타 플래터(덜매운맛)': '메인메뉴',\n",
    "       '브런치 2인 패키지 ': '메인메뉴',\n",
    "       '브런치 4인 패키지 ': '메인메뉴',\n",
    "       '브런치(대인) 주말': '메인메뉴',\n",
    "       '브런치(대인) 주중': '메인메뉴',\n",
    "       '브런치(어린이)': '메인메뉴',\n",
    "       '쉬림프 투움바 파스타': '메인메뉴',\n",
    "       '스텔라(무제한)': '주류',\n",
    "       '애플망고 에이드': '음료',\n",
    "       '얼그레이 하이볼': '주류',\n",
    "       '오븐구이 윙과 킬바사소세지': '메인메뉴',\n",
    "       '유자 하이볼': '주류',\n",
    "       '잭 애플 토닉': '주류',\n",
    "       '칠리 치즈 프라이': '추가메뉴',\n",
    "       '코카콜라': '음료',\n",
    "       '코카콜라(제로)': '음료',\n",
    "       '콥 샐러드': '추가메뉴',\n",
    "       '파스타면 추가(150g)': '추가메뉴',\n",
    "       '핑크레몬에이드': '음료',\n",
    "       'Cass Beer': '주류',\n",
    "       'Conference L1': '연회장 대여',\n",
    "       'Conference L2': '연회장 대여',\n",
    "       'Conference L3': '연회장 대여',\n",
    "       'Conference M1': '연회장 대여',\n",
    "       'Conference M8': '연회장 대여',\n",
    "       'Conference M9': '연회장 대여',\n",
    "       'Convention Hall': '연회장 대여',\n",
    "       'Cookie Platter': '디저트',\n",
    "       'Grand Ballroom': '연회장 대여',\n",
    "       'OPUS 2': '연회장 대여',\n",
    "       'Regular Coffee': '음료',\n",
    "       '골뱅이무침': '메인메뉴',\n",
    "       '돈목살 김치찌개 (밥포함)': '메인메뉴',\n",
    "       '로제 치즈떡볶이': '메인메뉴',\n",
    "       '마라샹궈': '메인메뉴',\n",
    "       '매콤 무뼈닭발&계란찜': '메인메뉴',\n",
    "       '모둠 돈육구이(3인)': '메인메뉴',\n",
    "       '삼겹살추가 (200g)': '추가메뉴',\n",
    "       '야채추가': '추가메뉴',\n",
    "       '왕갈비치킨': '메인메뉴',\n",
    "       '주먹밥 (2ea)': '추가메뉴',\n",
    "       '공깃밥(추가)': '추가메뉴',\n",
    "       '구슬아이스크림': '디저트',\n",
    "       '단체식 13000(신)': '메인메뉴',\n",
    "       '단체식 18000(신)': '메인메뉴',\n",
    "       '돼지고기 김치찌개': '메인메뉴',\n",
    "       '복숭아 아이스티': '음료',\n",
    "       '새우 볶음밥': '메인메뉴',\n",
    "       '새우튀김 우동': '메인메뉴',\n",
    "       '샷 추가': '추가메뉴',\n",
    "       '수제 등심 돈까스': '메인메뉴',\n",
    "       '아메리카노(HOT)': '음료',\n",
    "       '아메리카노(ICE)': '음료',\n",
    "       '약 고추장 돌솥비빔밥': '메인메뉴',\n",
    "       '어린이 돈까스': '메인메뉴',\n",
    "       '오픈푸드': '기타',\n",
    "       '진사골 설렁탕': '메인메뉴',\n",
    "       '짜장면': '메인메뉴',\n",
    "       '짜장밥': '메인메뉴',\n",
    "       '짬뽕': '메인메뉴',\n",
    "       '짬뽕밥': '메인메뉴',\n",
    "       '치즈돈까스': '메인메뉴',\n",
    "       '카페라떼(HOT)': '음료',\n",
    "       '카페라떼(ICE)': '음료',\n",
    "       '한상 삼겹구이 정식(2인) 소요시간 약 15~20분': '메인메뉴',\n",
    "       '꼬치어묵': '메인메뉴',\n",
    "       '떡볶이': '메인메뉴',\n",
    "       '생수': '음료',\n",
    "       '치즈 핫도그': '디저트',\n",
    "       '페스츄리 소시지': '디저트',\n",
    "       '단호박 식혜 ': '음료',\n",
    "       '병천순대': '메인메뉴',\n",
    "       '참살이 막걸리': '주류',\n",
    "       '찹쌀식혜': '음료',\n",
    "       '해물파전': '메인메뉴',\n",
    "       '메밀미숫가루': '음료',\n",
    "       '아메리카노 HOT': '음료',\n",
    "       '아메리카노 ICE': '음료',\n",
    "       '카페라떼 ICE': '음료',\n",
    "       '현미뻥스크림': '디저트'\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "6bad37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the menu_category to df\n",
    "train['menu_category'] = train['메뉴명'].map(menu_category)\n",
    "train['menu_category'], uniques = pd.factorize(train['menu_category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4515de",
   "metadata": {},
   "source": [
    "# avg_sales_all_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "1bee03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_avg = train.groupby('메뉴명')['매출수량'].mean()\n",
    "train['avg_sales_all_days'] = train['메뉴명'].map(menu_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3dbd6",
   "metadata": {},
   "source": [
    "# Feature about zero\n",
    "1. avg_sales_nonzero_days\n",
    "2. zero_sales_day_ratio\n",
    "3. is_sparse_menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "9346d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_avg = train[train['매출수량'] > 0].groupby('메뉴명')['매출수량'].mean()\n",
    "# Assign the nonzero_avg to df\n",
    "train['avg_sales_nonzero_days'] = train['메뉴명'].map(nonzero_avg)\n",
    "# 매출수량 > 0인 경우만 메뉴별 분산 계산\n",
    "nonzero_var = (\n",
    "    train[train['매출수량'] > 0]\n",
    "    .groupby('메뉴명')['매출수량']\n",
    "    .var()   # ← 분산\n",
    ")\n",
    "\n",
    "# train에 새로운 칼럼으로 추가\n",
    "train['var_sales_nonzero_days'] = train['메뉴명'].map(nonzero_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "3600cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_ratio = train.groupby('메뉴명')['매출수량'].apply(lambda x: (x.eq(0).sum() / len(x)) * 100)\n",
    "\n",
    "# Assign the zero_ratio to df\n",
    "train['zero_sales_day_ratio'] = train['메뉴명'].map(zero_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "0c2e5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['is_sparse_menu'] = np.where(train['zero_sales_day_ratio'] > 50, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934441b6",
   "metadata": {},
   "source": [
    "# Feature about 연회장\n",
    "1. banquet_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "8e0a61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_연회장 = train[train['영업장명']=='연회장'].pivot_table(index='영업일자',columns='메뉴명',values='매출수량', aggfunc = 'sum').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "3eaa823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_연회장['연회장 대여'] = df_연회장[['Conference L1','Conference L2','Conference L3','Conference M1','Conference M8','Conference M9','Convention Hall','Grand Ballroom','OPUS 2']].sum(axis=1)\n",
    "df_연회장['음료 및 쿠키'] = df_연회장[['Cookie Platter','Cass Beer','Regular Coffee']].sum(axis=1)\n",
    "df_연회장['음식'] = df_연회장[['골뱅이무침','공깃밥','돈목살 김치찌개 (밥포함)','로제 치즈떡볶이','마라샹궈','매콤 무뼈닭발&계란찜','모둠 돈육구이(3인)','삼겹살추가 (200g)','야채추가','왕갈비치킨','주먹밥 (2ea)']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "8b86fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def banquet_type(row):\n",
    "    if row['연회장 대여'] > 0 and row['음식'] == 0 and row['음료 및 쿠키'] == 0:\n",
    "        return 1 # 대여만\n",
    "    elif row['연회장 대여'] == 0 and row['음식'] == 0 and row['음료 및 쿠키'] > 0:\n",
    "        return 2 # 음료및쿠키만\n",
    "    elif row['연회장 대여'] == 0 and row['음식'] > 0 and row['음료 및 쿠키'] == 0:\n",
    "        return 3 # 음식만\n",
    "    elif row['연회장 대여'] > 0 and row['음식'] == 0 and row['음료 및 쿠키'] > 0:\n",
    "        return 4 # 대여+음료및쿠키\n",
    "    elif row['연회장 대여'] > 0 and row['음식'] > 0 and row['음료 및 쿠키'] == 0:\n",
    "        return 5 # 대여+음식\n",
    "    elif row['연회장 대여'] == 0 and row['음식'] > 0 and row['음료 및 쿠키'] > 0:\n",
    "        return 6 # 음식+음료및쿠키\n",
    "    elif row['연회장 대여'] > 0 and row['음식'] > 0 and row['음료 및 쿠키'] > 0:\n",
    "        return 7 # 대여+음료및쿠키+음식\n",
    "    else:\n",
    "        return 0 # 연회장 총매출이 0인경우\n",
    "df_연회장['banquet_type'] = df_연회장.apply(banquet_type, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b97523",
   "metadata": {},
   "source": [
    "# Features\n",
    "1. is_drink\n",
    "2. is_alcohol\n",
    "3. is_set_menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "700775b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drink_keywords = ['콜라', '스프라이트', '제로콜라', '자몽리치에이드', '애플망고 에이드', '핑크레몬에이드', '아메리카노',\n",
    "#                   '식혜', '메밀미숫가루', '아메리카노', '카페라떼', '복숭아 아이스티','샷 추가',\n",
    "#                   '생수']\n",
    "\n",
    "# alcohol_keywords = ['Gls.Sileni', 'Gls.미션 서드', '미션 서드 카메르네 쉬라', '하이네켄', '막걸리',\n",
    "#                     '와인', '버드와이저', '스텔라', '하이볼', '잭 애플 토닉', '참이슬', '소주', '처음처럼',\n",
    "#                     '카스', '테라', '칵테일', 'Cass']\n",
    "\n",
    "# set_keywords = ['정식']\n",
    "\n",
    "# train['is_drink'] = train['영업장명_메뉴명'].apply(\n",
    "#     lambda x: 1 if any(keyword in str(x) for keyword in drink_keywords) else 0\n",
    "# )\n",
    "\n",
    "# train['is_alcohol'] = train['영업장명_메뉴명'].apply(\n",
    "#     lambda x: 1 if (\n",
    "#         any(keyword in str(x) for keyword in alcohol_keywords)\n",
    "#         and '컵' not in str(x)\n",
    "#     ) else 0\n",
    "# )\n",
    "\n",
    "# train['is_set_menu'] = train['영업장명_메뉴명'].apply(\n",
    "#     lambda x: 1 if (\n",
    "#         any(keyword in str(x) for keyword in set_keywords)\n",
    "#     ) else 0\n",
    "# )\n",
    "\n",
    "# #매출수량이 문자열이면 숫자로 변환\n",
    "# train['매출수량'] = pd.to_numeric(train['매출수량'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec5dbe",
   "metadata": {},
   "source": [
    "# demand_volatility, demand_stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "779a5d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_stats=(\n",
    "    train.groupby('영업장명_메뉴명')['매출수량']\n",
    "    .agg(['mean','std'])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "menu_stats['demand_volatility']=menu_stats['std']/menu_stats['mean']\n",
    "# menu_stats['demand_stability']=1/menu_stats['demand_volatility']\n",
    "\n",
    "menu_stats.rename(columns={'mean':'평균매출수량','std':'표준편차'},inplace=True)\n",
    "\n",
    "# Merge menu_stats back to df\n",
    "train=train.merge(menu_stats[['영업장명_메뉴명', 'demand_volatility']], on='영업장명_메뉴명',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f169d",
   "metadata": {},
   "source": [
    "# Add New"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4f156",
   "metadata": {},
   "source": [
    "# **데이터 전처리**\n",
    "\n",
    "1. **is_spike**: 전식당, 변할 수 있는 가능성을 학습하게끔 하는 것, 당일 수치가 최근 7일간 평균 + 2 × 표준편차보다 크면 1, 아니면 0, 위로 갑자기 튀는 것을 포착\n",
    "2. **is_drop**: 전식당, 변할 수 있는 가능성을 학습하게끔 하는 것, 당일 수치가 최근 7일간 평균 − 2 × 표준편차보다 작으면 1, 아니면 0, 아래로 갑자기 튀는 것을 포착\n",
    "3. **is_weekday_price**: 미라시아, 요금제가 주중 기준인지 여부 구분\t메뉴명에 '주중'이 포함되면 1, 아니면 0, 주중 요금이 적용된 메뉴인지 여부\n",
    "4. **is_weekend_price**: 미라시아, 요금제가 주말 기준인지 여부 구분\t메뉴명에 '주말'이 포함되면 1, 아니면 0, 주말 요금이 적용된 메뉴인지 여부\n",
    "5. **seasonal_index**: 전식당,\n",
    "\n",
    "    • 1분기 (Q1): 1월 1일부터 3월 31일까지\n",
    "\n",
    "    • 2분기 (Q2): 4월 1일부터 6월 30일까지\n",
    "\n",
    "    • 3분기 (Q3): 7월 1일부터 9월 30일까지\n",
    "\n",
    "    • 4분기 (Q4): 10월 1일부터 12월 31일까지\n",
    "\n",
    "    월별 또는 분기별 매출 패턴 분석하여 생성, 분기별 매출 수치화\n",
    "6. **미라시아 단체 관련 변수( brunch_flag, hallroom_flag)** :\n",
    "    \n",
    "    6-1. **brunch_flag**: 단체 브런치 메뉴 매출이 생긴 날의 플래그, 연회장_룸타입에만 플래그를 세운다.\n",
    "\n",
    "    6-2. **hallroom_flag**: 연회장_룸타입 매출이 생긴 날의 플래그, (단체)브런치주중 36,000 에만 플래그를 세운다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed6885",
   "metadata": {},
   "source": [
    "# quarter_index(all)\n",
    "\n",
    "생성 목적: 분기별 매출 수치화\n",
    "\n",
    "생성 방법:\n",
    "\n",
    "• 1분기 (Q1): 1월 1일부터 3월 31일까지\n",
    "\n",
    "• 2분기 (Q2): 4월 1일부터 6월 30일까지\n",
    "\n",
    "• 3분기 (Q3): 7월 1일부터 9월 30일까지\n",
    "\n",
    "• 4분기 (Q4): 10월 1일부터 12월 31일까지\n",
    "\n",
    "분기별 매출 패턴 분석하여 누적합으로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "0f23773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_solar_term(\n",
    "    train: pd.DataFrame,\n",
    "    terms: pd.DataFrame,\n",
    "    date_col: str = \"영업일자\",\n",
    "    term_date_col: str = \"locdate\",\n",
    "    term_name_col: str = \"solar_term\",\n",
    "    out_col: str = \"solar_term\"\n",
    ") -> pd.DataFrame:\n",
    "    df = train.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    terms = terms.copy()\n",
    "    terms[term_date_col] = pd.to_datetime(terms[term_date_col], errors='coerce')\n",
    "\n",
    "    # 절기 구간 만들기\n",
    "    terms = terms.sort_values(term_date_col).reset_index(drop=True)\n",
    "    terms['end_date'] = terms[term_date_col].shift(-1) - pd.Timedelta(days=1)\n",
    "    terms.loc[terms.index[-1], 'end_date'] = df[date_col].max()\n",
    "\n",
    "    # ★ 절기명 컬럼을 임시 이름으로 바꿔서 머지\n",
    "    tmp_col = \"_term_name_tmp\"\n",
    "    right = terms[[term_date_col, term_name_col, 'end_date']].rename(columns={term_name_col: tmp_col})\n",
    "\n",
    "    merged = pd.merge_asof(\n",
    "        df.sort_values(date_col),\n",
    "        right.sort_values(term_date_col),\n",
    "        left_on=date_col,\n",
    "        right_on=term_date_col,\n",
    "        direction='backward'\n",
    "    )\n",
    "\n",
    "    in_range = merged[date_col] <= merged['end_date']\n",
    "    merged[out_col] = merged[tmp_col].where(in_range)\n",
    "\n",
    "    # 임시 컬럼만 삭제 (out_col은 유지)\n",
    "    merged = merged.drop(columns=[term_date_col, 'end_date', tmp_col]).sort_index()\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "d648e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0) 날짜 보정\n",
    "# if df['영업일자'].dtype == 'O':\n",
    "#     df['영업일자'] = pd.to_datetime(df['영업일자'], errors='coerce')\n",
    "\n",
    "# 1) '분기' → 'quarter'로 변경 (없으면 새로 생성)\n",
    "#     Q1, Q2, Q3, Q4 형식으로 생성\n",
    "def get_quarter_code(m):\n",
    "        if m in (1,2,3): return 0\n",
    "        if m in (4,5,6): return 1\n",
    "        if m in (7,8,9): return 2\n",
    "        return 3\n",
    "train['quarter'] = train['month'].apply(get_quarter_code)\n",
    "train['season'] = train.apply(\n",
    "    lambda x: 0 if x['month'] in [3, 4, 5] else\n",
    "              (1 if x['month'] in [6, 7, 8] else\n",
    "               (2 if x['month'] in [9, 10, 11] else 3)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# terms 예시: columns=['locdate','절기'] 라면 term_name_col='절기'로 지정\n",
    "train = attach_solar_term(\n",
    "    train,\n",
    "    terms,                   # 절기 lookup DF\n",
    "    date_col=\"영업일자\",\n",
    "    term_date_col=\"locdate\",\n",
    "    term_name_col=\"solar_term\",    # 실제 컬럼명 맞추기!\n",
    "    out_col=\"solar_term\"     # train에 새로 생길 이름\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "0ba8b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumsum(train, col, newcol):    \n",
    "# 0) 날짜 보정\n",
    "    if train['영업일자'].dtype == 'O':\n",
    "        train['영업일자'] = pd.to_datetime(train['영업일자'], errors='coerce')\n",
    "\n",
    "    # 2) 임시로 매장/메뉴 분리(컬럼 추가 X)\n",
    "    keys = train['영업장명_메뉴명'].str.split('_', n=1, expand=True)\n",
    "    g_store, g_menu = keys[0], keys[1]\n",
    "\n",
    "    # 3) 그룹별 전일까지 누적합 → 전체 열에 한 번에 대입 (정수 고정)\n",
    "    #    - 정렬은 누적 순서만 위해 잠깐 사용, 결과는 원래 인덱스로 돌아옴\n",
    "    train_sorted = train.sort_values([col, '영업일자']).copy()\n",
    "    seasonal_series = (\n",
    "        train_sorted\n",
    "        .groupby([g_store.reindex(train_sorted.index),\n",
    "                    g_menu.reindex(train_sorted.index),\n",
    "                    train_sorted[col]], sort=False)['매출수량']\n",
    "        .transform(lambda s: s.shift(1).cumsum())\n",
    "    )\n",
    "\n",
    "    train[newcol] = (\n",
    "        seasonal_series.reindex(train_sorted.index)\n",
    "                    .reindex(train.index)\n",
    "                    .fillna(0)\n",
    "                    .astype('int64')\n",
    "    )\n",
    "    return train\n",
    "train = cumsum(train, 'quarter', 'quarter_sum')\n",
    "train = cumsum(train, 'season', 'season_sum')\n",
    "train = cumsum(train, 'solar_term', 'solar_term_sum')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4ec20",
   "metadata": {},
   "source": [
    "# **미라시아 단체 관련 변수( brunch_flag, hallroom_flag)**\n",
    "\n",
    "**brunch_flag**: 단체 브런치 메뉴 매출이 생긴 날의 플래그, 연회장_룸타입('Grand Ballroom', 'Convention Hall', 'Conference L', 'Conference M', 'OPUS 2')에만 플래그를 세운다.\n",
    "\n",
    "**hallroom_flag**: 연회장_룸타입 매출이 생긴 날의 플래그, (단체)브런치주중 36,000 에만 플래그를 세운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "40463ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = '미라시아_(단체)브런치주중 36,000'\n",
    "HALL_ROOMS = {'Grand Ballroom', 'Convention Hall', 'Conference L', 'Conference M', 'OPUS'}\n",
    "\n",
    "# 날짜형 변환\n",
    "if train['영업일자'].dtype == 'O':\n",
    "    train['영업일자'] = pd.to_datetime(train['영업일자'], errors='coerce')\n",
    "\n",
    "# '영업장명_메뉴명' 분리\n",
    "tokens = train['영업장명_메뉴명'].str.split('_', n=1, expand=True)\n",
    "store0 = tokens[0].astype(str).str.strip()   # 예: '연회장', '미라시아', ...\n",
    "store1 = tokens[1].astype(str).str.strip()   # 예: 'Grand Ballroom', '(단체)브런치주중 36,000', ...\n",
    "\n",
    "# 1) 연회장 매출 발생 날짜\n",
    "hall_mask = (store0.eq('연회장')) & (store1.isin(HALL_ROOMS)) & (train['매출수량'] > 0)\n",
    "hall_dates = train.loc[hall_mask, '영업일자'].unique()\n",
    "\n",
    "# 2) 브런치 매출 발생 날짜\n",
    "brunch_mask = train['영업장명_메뉴명'].eq(TARGET) & (train['매출수량'] > 0)\n",
    "brunch_dates = train.loc[brunch_mask, '영업일자'].unique()\n",
    "\n",
    "# 3) 플래그 생성 (반대로 반영)\n",
    "# 초기화: 전부 0\n",
    "train['brunch_flag'] = 0      # ← 연회장 라인에 찍힘 (브런치 매출 발생일 기준)\n",
    "train['hallroom_flag'] = 0    # ← 미라시아 단체 브런치 라인에 찍힘 (연회장 매출 발생일 기준)\n",
    "\n",
    "# A) 단체 브런치 매출 발생일 → \"연회장_*\" 행에 brunch_flag=1\n",
    "train.loc[hall_mask & train['영업일자'].isin(brunch_dates), 'brunch_flag'] = 1\n",
    "\n",
    "# B) 연회장 매출 발생일 → \"미라시아_(단체)브런치주중 36,000\" 행에 hallroom_flag=1\n",
    "target_row_mask = train['영업장명_메뉴명'].eq(TARGET)\n",
    "train.loc[target_row_mask & train['영업일자'].isin(hall_dates), 'hallroom_flag'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "481a19e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add features from the 'train' DataFrame to 'df'\n",
    "# Ensure '영업일자' is datetime in both dataframes for merging\n",
    "train['영업일자'] = pd.to_datetime(train['영업일자'], errors='coerce')\n",
    "\n",
    "# Merge based on '영업일자' and '영업장명_메뉴명'\n",
    "# Recreate '영업장명_메뉴명' in train for merging if it was dropped\n",
    "if '영업장명_메뉴명' not in train.columns:\n",
    "    train['영업장명_메뉴명'] = train['영업장명'] + '_' + train['메뉴명']\n",
    "\n",
    "# Add banquet_type from df_연회장\n",
    "# Ensure '영업일자' is datetime in df_연회장\n",
    "df_연회장['영업일자'] = pd.to_datetime(df_연회장['영업일자'], errors='coerce')\n",
    "\n",
    "train = pd.merge(train, df_연회장[['영업일자', 'banquet_type']], on='영업일자', how='left')\n",
    "\n",
    "# Fill NaN values in banquet_type with 0 (assuming 0 means no banquet)\n",
    "train['banquet_type'] = train['banquet_type'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5487a9",
   "metadata": {},
   "source": [
    "# Store trian.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "8e0703ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['영업일자', '영업장명_메뉴명', '매출수량', 'time_idx', 'year', 'month', 'day', '영업장명',\n",
      "       '메뉴명', 'weekday', 'is_holiday', 'is_weekend', 'non_work', 'is_sandwich',\n",
      "       'weekday_score', 'is_before_holiday', 'is_after_holiday', 'menu_rank',\n",
      "       'menu_category', 'avg_sales_all_days', 'avg_sales_nonzero_days',\n",
      "       'var_sales_nonzero_days', 'zero_sales_day_ratio', 'demand_volatility',\n",
      "       'quarter', 'season', 'solar_term', 'quarter_sum', 'season_sum',\n",
      "       'solar_term_sum', 'brunch_flag', 'hallroom_flag', 'banquet_type'],\n",
      "      dtype='object')\n",
      "train DataFrame이 output.csv로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# train.drop(columns=['영업장명','메뉴명'], errors='ignore', inplace=True)\n",
    "train[['영업장명', '메뉴명']] = train['영업장명_메뉴명'].str.split('_', expand=True)\n",
    "# train.drop(columns=['day_of_quarter'], inplace=True, errors='ignore')\n",
    "# train.drop(columns=['cum_share_ref'], inplace=True, errors='ignore')\n",
    "# train DataFrame을 CSV로 저장 (모든 feature 포함)\n",
    "train = train.sort_values(['영업장명_메뉴명', '영업일자']).reset_index(drop=True)\n",
    "train.to_csv(\"re_train_06.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(train.columns)\n",
    "\n",
    "print(\"train DataFrame이 output.csv로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708aa8b6",
   "metadata": {},
   "source": [
    "# Task\n",
    "Apply the same feature engineering steps (date features, holiday features, spike/drop, seasonal index, banquet type, etc.) that were applied to the training data to the following test files: \"TEST_01.csv\", \"TEST_02.csv\", \"TEST_03.csv\", \"TEST_04.csv\", \"TEST_05.csv\", \"TEST_06.csv\", \"TEST_07.csv\", \"TEST_08.csv\", \"TEST_09.csv\". Ensure that all engineered features are added as new columns to the respective DataFrames loaded from these files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9524c",
   "metadata": {},
   "source": [
    "## Identify test files\n",
    "\n",
    "### Subtask:\n",
    "Create a list of all the test file paths (`TEST_01.csv` to `TEST_09.csv`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7ae7b",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "Create a list containing the file paths for the test datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "c1ddf32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TEST_00.csv', 'TEST_01.csv', 'TEST_02.csv', 'TEST_03.csv', 'TEST_04.csv', 'TEST_05.csv', 'TEST_06.csv', 'TEST_07.csv', 'TEST_08.csv', 'TEST_09.csv']\n"
     ]
    }
   ],
   "source": [
    "test_files = [f\"TEST_{i:02d}.csv\" for i in range(0, 10)] # Changed range from 1 to 0 to include TEST_00\n",
    "print(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe34e70",
   "metadata": {},
   "source": [
    "## Define feature engineering function\n",
    "\n",
    "### Subtask:\n",
    "Create a function that takes a DataFrame (like the one loaded from a test file) and applies all the necessary feature engineering steps (date features, holiday features, spike/drop, seasonal index, banquet type, etc.) to it, returning the processed DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f4ebe",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "Define a function `engineer_features` that takes a DataFrame and applies all the feature engineering steps. This function will include date features, holiday features, spike/drop detection, weekday/weekend price flags, seasonal index, brunch/hallroom flags, and banquet type merging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df_test, holiday_df, banquet_df, terms_df=None, terms_csv_path=None):\n",
    "    \"\"\"Applies feature engineering steps to a test DataFrame.\n",
    "       terms_df: (선택) 절기 테이블 DataFrame (cols: ['locdate','solar_term'])\n",
    "       terms_csv_path: (선택) terms_df가 없을 때 읽어올 CSV 경로\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    df_test[['영업장명', '메뉴명']] = df_test['영업장명_메뉴명'].str.split('_', expand=True)\n",
    "\n",
    "\n",
    "    # --- 기본 날짜 피처 ---\n",
    "    df_test = df_test.copy()\n",
    "    df_test['영업일자'] = pd.to_datetime(df_test['영업일자'], errors='coerce')\n",
    "    df_test['time_idx'] = (df_test['영업일자'] - df_test['영업일자'].min()).dt.days\n",
    "    df_test['year']    = df_test['영업일자'].dt.year.astype(int)\n",
    "    df_test['month']   = df_test['영업일자'].dt.month.astype(int)\n",
    "    df_test['day']     = df_test['영업일자'].dt.day.astype(int)\n",
    "    df_test['weekday'] = df_test['영업일자'].dt.weekday.astype(int)\n",
    "\n",
    "    # --- season 코드 (이미 사용 중) ---\n",
    "    def get_season_code(m):\n",
    "        if m in (12,1,2): return 0\n",
    "        if m in (3,4,5):  return 1\n",
    "        if m in (6,7,8):  return 2\n",
    "        return 3\n",
    "    df_test['season'] = df_test['month'].apply(get_season_code)\n",
    "    def get_quarter_code(m):\n",
    "        if m in (1,2,3): return 0\n",
    "        if m in (4,5,6): return 1\n",
    "        if m in (7,8,9): return 2\n",
    "        return 3\n",
    "    df_test['quarter'] = df_test['month'].apply(get_quarter_code)\n",
    "    df_test = attach_solar_term(\n",
    "        df_test,\n",
    "        terms,                   # 절기 lookup DF\n",
    "        date_col=\"영업일자\",\n",
    "        term_date_col=\"locdate\",\n",
    "        term_name_col=\"solar_term\",    # 실제 컬럼명 맞추기!\n",
    "        out_col=\"solar_term\"     # train에 새로 생길 이름\n",
    "    )\n",
    "\n",
    "    def attach_avg_by_keys(train, df_test, value_col, out_col=None,\n",
    "                        keys=('영업장명_메뉴명','month','day')):\n",
    "        # train에서 키별 평균 테이블\n",
    "        mp = (train.groupby(list(keys), as_index=False)[value_col]\n",
    "                    .mean()\n",
    "                    .rename(columns={value_col: out_col or value_col}))\n",
    "        # test에 merge\n",
    "        df_test = df_test.merge(mp, on=list(keys), how='left')\n",
    "        return df_test\n",
    "\n",
    "    # 사용 예시: 각 *_sum 컬럼의 평균을 동일 키로 test에 붙이기\n",
    "    df_test = attach_avg_by_keys(train, df_test, 'quarter_sum')\n",
    "    df_test = attach_avg_by_keys(train, df_test, 'season_sum')\n",
    "    df_test = attach_avg_by_keys(train, df_test, 'solar_term_sum')\n",
    "\n",
    "    # 2. Holiday features\n",
    "    holiday_df['locdate'] = pd.to_datetime(holiday_df['locdate'])\n",
    "    df_test = pd.merge(\n",
    "        df_test,\n",
    "        holiday_df[['locdate', 'isHoliday']],\n",
    "        how='left',\n",
    "        left_on='영업일자',\n",
    "        right_on='locdate'\n",
    "    )\n",
    "    df_test['is_holiday'] = df_test['isHoliday'].fillna('N').apply(lambda x: 1 if x == 'Y' else 0)\n",
    "    df_test = df_test.drop(['locdate', 'isHoliday'], axis=1)  # 둘 다 삭제\n",
    "    df_test['is_weekend'] = df_test['weekday'].apply(lambda x: 1 if x in [5, 6] else 0)\n",
    "\n",
    "    df_test[\"non_work\"] = ((df_test[\"is_holiday\"] == 1) | (df_test[\"is_weekend\"] == 1)).astype(int)\n",
    "\n",
    "    # train에서 menu_rank 매핑 테이블 추출 (중복 제거)\n",
    "    menu_rank_map = train[['영업장명_메뉴명', 'menu_rank']].drop_duplicates()\n",
    "\n",
    "    # df_test에 merge\n",
    "    df_test = df_test.merge(\n",
    "        menu_rank_map,\n",
    "        on='영업장명_메뉴명',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # 0/1로 정의: non_work = (주말 or 휴일) 이면 1, 평일이면 0\n",
    "    df_test['영업일자'] = pd.to_datetime(df_test['영업일자'])\n",
    "\n",
    "    # 1) 날짜 단위 대표 non_work 만들기 (하루에 여러 행이면 max로 대표)\n",
    "    daily = (df_test.groupby('영업일자', as_index=False)['non_work']\n",
    "                .max()\n",
    "                .sort_values('영업일자'))\n",
    "\n",
    "    # 2) 샌드위치 데이: 오늘은 근무일(0)이고, 전/다음날은 쉬는날(1)\n",
    "    daily['is_sandwich'] = (\n",
    "        (daily['non_work'].eq(0)) &\n",
    "        (daily['non_work'].shift(1,  fill_value=0).eq(1)) &\n",
    "        (daily['non_work'].shift(-1, fill_value=0).eq(1))\n",
    "    ).astype(int)\n",
    "\n",
    "    # 3) 원본 df_test에 날짜로 merge (기존 컬럼 있으면 덮어쓰기)\n",
    "    df_test = df_test.drop(columns=['is_sandwich'], errors='ignore') \\\n",
    "                    .merge(daily[['영업일자','is_sandwich']], on='영업일자', how='left')\n",
    "    # 0/1로 되어 있다고 가정: non_work = (주말 or 휴일) 1, 평일 0\n",
    "    df_test['영업일자'] = pd.to_datetime(df_test['영업일자'])\n",
    "\n",
    "    # 1) 날짜 단위 non_work(하루에 여러 행이면 max로 대표)\n",
    "    daily = (df_test.groupby('영업일자', as_index=False)['non_work']\n",
    "                .max()\n",
    "                .sort_values('영업일자'))\n",
    "\n",
    "    # 2) 날짜 기준으로 before/after 플래그 계산\n",
    "    daily['is_before_holiday'] = (\n",
    "        (daily['non_work'].eq(0)) &\n",
    "        (daily['non_work'].shift(-1, fill_value=0).eq(1))\n",
    "    ).astype(int)\n",
    "\n",
    "    daily['is_after_holiday'] = (\n",
    "        (daily['non_work'].eq(0)) &\n",
    "        (daily['non_work'].shift(1, fill_value=0).eq(1))\n",
    "    ).astype(int)\n",
    "\n",
    "    # 3) 원본 df_test에 날짜로 merge\n",
    "    df_test = df_test.merge(\n",
    "        daily[['영업일자', 'is_before_holiday', 'is_after_holiday']],\n",
    "        on='영업일자', how='left'\n",
    "    )\n",
    "\n",
    "# 필요하면 non_work 컬럼 정리\n",
    "# train = train.drop(columns=['non_work'], errors='ignore')\n",
    "    df_test = df_test.drop(columns=['non_work'], errors='ignore')\n",
    "\n",
    "    # train에서 필요한 칼럼만 매핑 테이블로 추출 (중복 제거)\n",
    "    weekday_map = train[['영업장명_메뉴명', 'weekday', 'weekday_score']].drop_duplicates()\n",
    "\n",
    "    # merge로 붙이기\n",
    "    df_test = df_test.merge(\n",
    "        weekday_map,\n",
    "        on=['영업장명_메뉴명', 'weekday'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # 3. lookup features\n",
    "\n",
    "    feat_cols = ['영업장명_메뉴명',\n",
    "             'menu_category','avg_sales_all_days','avg_sales_nonzero_days',\n",
    "             'zero_sales_day_ratio','demand_volatility', 'var_sales_nonzero_days']\n",
    "\n",
    "    train_feats = (train[feat_cols]\n",
    "               .groupby('영업장명_메뉴명', as_index=False)\n",
    "               .agg('first'))  # 필요시 mean/max로 변경\n",
    "\n",
    "    df_test = df_test.merge(train_feats, on='영업장명_메뉴명',\n",
    "                        how='left', validate='many_to_one')\n",
    "\n",
    "    # 6. Brunch/Hallroom flags\n",
    "    TARGET = '미라시아_(단체)브런치주중 36,000'\n",
    "    HALL_ROOMS = {'Grand Ballroom', 'Convention Hall', 'Conference L', 'Conference M', 'OPUS'}\n",
    "\n",
    "    tokens = df_test['영업장명_메뉴명'].str.split('_', n=1, expand=True)\n",
    "    store0 = tokens[0].astype(str).str.strip()\n",
    "    store1 = tokens[1].astype(str).str.strip()\n",
    "\n",
    "    # Recalculate hall_dates and brunch_dates from the *original train* data (assuming 'train' df is available globally)\n",
    "    if 'train' in globals():\n",
    "        train_tokens = train['영업장명_메뉴명'].str.split('_', n=1, expand=True)\n",
    "        train_store0 = train_tokens[0].astype(str).str.strip()\n",
    "        train_store1 = train_tokens[1].astype(str).str.strip()\n",
    "\n",
    "        train_hall_mask = (train_store0.eq('연회장')) & (train_store1.isin(HALL_ROOMS)) & (train['매출수량'] > 0)\n",
    "        hall_dates = train.loc[train_hall_mask, '영업일자'].unique()\n",
    "\n",
    "        train_brunch_mask = train['영업장명_메뉴명'].eq(TARGET) & (train['매출수량'] > 0)\n",
    "        brunch_dates = train.loc[train_brunch_mask, '영업일자'].unique()\n",
    "    else:\n",
    "        # Fallback or error handling if train data is not available\n",
    "        print(\"Warning: 'train' DataFrame not found. Cannot calculate brunch_dates and hall_dates.\")\n",
    "        hall_dates = []\n",
    "        brunch_dates = []\n",
    "\n",
    "\n",
    "    if 'brunch_flag' not in df_test.columns:\n",
    "        df_test['brunch_flag'] = 0\n",
    "    if 'hallroom_flag' not in df_test.columns:\n",
    "        df_test['hallroom_flag'] = 0\n",
    "\n",
    "    test_hall_mask = (store0.eq('연회장')) & (store1.isin(HALL_ROOMS))\n",
    "    test_target_row_mask = df_test['영업장명_메뉴명'].eq(TARGET)\n",
    "\n",
    "    df_test.loc[test_hall_mask & df_test['영업일자'].isin(brunch_dates), 'brunch_flag'] = 1\n",
    "    df_test.loc[test_target_row_mask & df_test['영업일자'].isin(hall_dates), 'hallroom_flag'] = 1\n",
    "\n",
    "\n",
    "    # 7. Banquet type\n",
    "    # Ensure '영업일자' is datetime in banquet_df\n",
    "    banquet_df['영업일자'] = pd.to_datetime(banquet_df['영업일자'], errors='coerce')\n",
    "    df_test = pd.merge(df_test, banquet_df[['영업일자', 'banquet_type']], on='영업일자', how='left')\n",
    "    df_test['banquet_type'] = df_test['banquet_type'].fillna(0).astype(int)\n",
    "\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0baba5",
   "metadata": {},
   "source": [
    "## Process each test file\n",
    "\n",
    "### Subtask:\n",
    "Iterate through the list of test file paths. For each file:\n",
    "    - Load the CSV into a DataFrame.\n",
    "    - Apply the feature engineering function to the DataFrame.\n",
    "    - Store the processed DataFrame (e.g., in a dictionary or list).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8fe5f9",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "Iterate through the test files, apply the feature engineering function to each, and store the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "ed09f328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/garden/Desktop/lgaimers/Hackaton/DataProcessing/re_data_processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TEST_00.csv...\n",
      "Finished processing TEST_00.csv.\n",
      "Processing TEST_01.csv...\n",
      "Finished processing TEST_01.csv.\n",
      "Processing TEST_02.csv...\n",
      "Finished processing TEST_02.csv.\n",
      "Processing TEST_03.csv...\n",
      "Finished processing TEST_03.csv.\n",
      "Processing TEST_04.csv...\n",
      "Finished processing TEST_04.csv.\n",
      "Processing TEST_05.csv...\n",
      "Finished processing TEST_05.csv.\n",
      "Processing TEST_06.csv...\n",
      "Finished processing TEST_06.csv.\n",
      "Processing TEST_07.csv...\n",
      "Finished processing TEST_07.csv.\n",
      "Processing TEST_08.csv...\n",
      "Finished processing TEST_08.csv.\n",
      "Processing TEST_09.csv...\n",
      "Finished processing TEST_09.csv.\n",
      "\n",
      "Sample of processed TEST_00.csv:\n",
      "['영업일자', '영업장명_메뉴명', '매출수량', '영업장명', '메뉴명', 'time_idx', 'year', 'month', 'day', 'weekday', 'season', 'quarter', 'solar_term', 'quarter_sum', 'season_sum', 'solar_term_sum', 'is_holiday', 'is_weekend', 'menu_rank', 'is_sandwich', 'is_before_holiday', 'is_after_holiday', 'weekday_score', 'menu_category', 'avg_sales_all_days', 'avg_sales_nonzero_days', 'zero_sales_day_ratio', 'demand_volatility', 'var_sales_nonzero_days', 'brunch_flag', 'hallroom_flag', 'banquet_type']\n"
     ]
    }
   ],
   "source": [
    "processed_test_dfs = {}\n",
    "\n",
    "# Load necessary dataframes outside the loop\n",
    "print(os.getcwd())  # Ensure the current working directory is set correctly\n",
    "terms_df = pd.read_csv('../solar_term_2023_2025.csv')  # Load the terms CSV\n",
    "holiday_df = pd.read_csv('../holidays_2023_2025.csv')\n",
    "banquet_df_full = pd.read_csv('re_train_06.csv') # Load the processed train data\n",
    "\n",
    "# Prepare the banquet_df for merging (only date and type)\n",
    "banquet_df_for_merge = banquet_df_full[['영업일자', 'banquet_type']].drop_duplicates()\n",
    "\n",
    "for file_path in test_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    # Load the test data\n",
    "    df_test = pd.read_csv(f\"../../data/test/{file_path}\")\n",
    "\n",
    "    # Apply feature engineering\n",
    "    # Pass the necessary dataframes to the function\n",
    "    processed_df = engineer_features(df_test, holiday_df, banquet_df_for_merge, terms_df)\n",
    "\n",
    "    # Store the processed DataFrame\n",
    "    processed_test_dfs[file_path] = processed_df\n",
    "    print(f\"Finished processing {file_path}.\")\n",
    "\n",
    "# Display the first few rows of one of the processed dataframes to verify\n",
    "if processed_test_dfs:\n",
    "    first_file = list(processed_test_dfs.keys())[0]\n",
    "    print(f\"\\nSample of processed {first_file}:\")\n",
    "    print(processed_test_dfs[first_file].columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe2568",
   "metadata": {},
   "source": [
    "## Save processed test data (optional)\n",
    "\n",
    "### Subtask:\n",
    "Save each processed test DataFrame to a new CSV file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f97d77",
   "metadata": {},
   "source": [
    "**Reasoning**:\n",
    "Iterate through the processed test dataframes and save each one to a CSV file with an added suffix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "b23158e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed TEST_00.csv to ./re_test_processed_04/TEST_00_processed.csv\n",
      "Saved processed TEST_01.csv to ./re_test_processed_04/TEST_01_processed.csv\n",
      "Saved processed TEST_02.csv to ./re_test_processed_04/TEST_02_processed.csv\n",
      "Saved processed TEST_03.csv to ./re_test_processed_04/TEST_03_processed.csv\n",
      "Saved processed TEST_04.csv to ./re_test_processed_04/TEST_04_processed.csv\n",
      "Saved processed TEST_05.csv to ./re_test_processed_04/TEST_05_processed.csv\n",
      "Saved processed TEST_06.csv to ./re_test_processed_04/TEST_06_processed.csv\n",
      "Saved processed TEST_07.csv to ./re_test_processed_04/TEST_07_processed.csv\n",
      "Saved processed TEST_08.csv to ./re_test_processed_04/TEST_08_processed.csv\n",
      "Saved processed TEST_09.csv to ./re_test_processed_04/TEST_09_processed.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"./re_test_processed_04/\" # Save in the current directory\n",
    "\n",
    "for filename, df_processed in processed_test_dfs.items():\n",
    "    # Construct output filename by adding '_processed' before the extension\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    output_filename = f\"{base}_processed{ext}\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    df_processed.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "\n",
    "    print(f\"Saved processed {filename} to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
