{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 691,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "from matplotlib import rc\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import glob\n",
        "import os\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Read csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 692,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission = pd.read_csv('../../data/sample_submission.csv')\n",
        "holidays = pd.read_csv('../../data/holidays_2023_2025.csv')\n",
        "train = pd.read_csv('../../data/train/train.csv')\n",
        "# os.getcwd()\n",
        "terms = pd.read_csv('../solar_term_2023_2025.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature about date\n",
        "1. year\n",
        "2. month\n",
        "3. day\n",
        "4. weekday\n",
        "5. is_holiday\n",
        "6. is_sandwich\n",
        "7. is_before_holiday\n",
        "8. is_after_holiday\n",
        "9. is_weekend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 693,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcVcWuopB9Ki",
        "outputId": "d9172d6d-a9a6-4a3b-a8e7-90b8b19d0f55"
      },
      "outputs": [],
      "source": [
        "# 1) 날짜 범위\n",
        "dates = pd.date_range(start=\"2023-01-01\", end=\"2025-05-31\", freq=\"D\")\n",
        "\n",
        "# 연, 월, 일 col 생성\n",
        "train['영업일자'] = pd.to_datetime(train['영업일자'])\n",
        "train['time_idx'] = (train['영업일자'] - train['영업일자'].min()).dt.days\n",
        "train['year'] = train['영업일자'].dt.year\n",
        "train['month'] = train['영업일자'].dt.month\n",
        "train['day'] = train['영업일자'].dt.day\n",
        "# 영업장, 메뉴명 col 생성\n",
        "train[['영업장명', '메뉴명']] = train['영업장명_메뉴명'].str.split('_', expand=True)\n",
        "# 요일 col 생성\n",
        "train['weekday'] = train['영업일자'].dt.weekday.astype(int)\n",
        "\n",
        "# 공휴일 col 생성\n",
        "holidays[\"locdate\"] = pd.to_datetime(holidays[\"locdate\"])\n",
        "holiday_dates = set(holidays['locdate'])\n",
        "train['is_holiday'] = train['영업일자'].isin(holiday_dates).astype(int)\n",
        "\n",
        "# 5) isSandwich: 오늘은 평일(0)이고, 어제/내일이 모두 쉬는 날(1)인 경우 1\n",
        "train[\"is_sandwich\"] = 0\n",
        "train.loc[\n",
        "    (train[\"is_holiday\"] == 0) &\n",
        "    (train[\"is_holiday\"].shift(1) == 1) &\n",
        "    (train[\"is_holiday\"].shift(-1) == 1),\n",
        "    \"is_sandwich\"\n",
        "] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 694,
      "metadata": {
        "id": "8aj9OnEkD00R"
      },
      "outputs": [],
      "source": [
        "train['is_before_holiday'] = train['is_holiday'].shift(-1).fillna(0)\n",
        "train['is_before_holiday'] = train['is_before_holiday'].astype(int)\n",
        "train['is_after_holiday'] = train['is_holiday'].shift(1).fillna(0)\n",
        "train['is_after_holiday'] = train['is_after_holiday'].astype(int)\n",
        "train['is_weekend'] = train['weekday'].apply(lambda x: 1 if x in [5, 6] else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature about top menu --todo--"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 695,
      "metadata": {
        "id": "vV1n65DICuvY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/3t/x_vcdczj5fn_v1nnkppzjpwh0000gn/T/ipykernel_2586/2046610547.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g.nlargest(3, '매출수량'))\n"
          ]
        }
      ],
      "source": [
        "# 예시: train 데이터에 영업장명, 메뉴명, 매출수량 컬럼이 있다고 가정\n",
        "# train = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# 1) 영업장별 메뉴별 총 매출수량 계산\n",
        "menu_sales = (\n",
        "    train.groupby(['영업장명', '메뉴명'], as_index=False)['매출수량']\n",
        "         .sum()\n",
        ")\n",
        "\n",
        "# 2) 영업장별 매출수량 상위 3개 메뉴 추출\n",
        "top3_menus = (\n",
        "    menu_sales.groupby('영업장명')\n",
        "              .apply(lambda g: g.nlargest(3, '매출수량'))\n",
        "              .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# 3) Top3 여부를 dict 형태로 만들어 매핑\n",
        "top3_set = set(zip(top3_menus['영업장명'], top3_menus['메뉴명']))\n",
        "\n",
        "# 4) train에 is_popular 플래그 추가\n",
        "train['is_popular'] = train.apply(\n",
        "    lambda row: 1 if (row['영업장명'], row['메뉴명']) in top3_set else 0,\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# menu_ category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 696,
      "metadata": {},
      "outputs": [],
      "source": [
        "menu_category = {\n",
        "    '1인 수저세트': '기타',\n",
        "    'BBQ55(단체)': '메인메뉴',\n",
        "    '대여료 60,000원': '기타',\n",
        "    '대여료 30,000원': '기타',\n",
        "       '대여료 90,000원': '기타',\n",
        "       '본삼겹 (단품,실내)': '메인메뉴',\n",
        "       '스프라이트 (단체)': '음료',\n",
        "       '신라면': '추가메뉴',\n",
        "       '쌈야채세트': '추가메뉴',\n",
        "       '쌈장': '추가메뉴',\n",
        "       '육개장 사발면': '추가메뉴',\n",
        "       '일회용 소주컵': '기타',\n",
        "       '일회용 종이컵': '기타',\n",
        "       '잔디그늘집 대여료 (12인석)': '기타',\n",
        "       '잔디그늘집 대여료 (6인석)': '기타',\n",
        "       '잔디그늘집 의자 추가': '기타',\n",
        "       '참이슬 (단체)': '주류',\n",
        "       '친환경 접시 14cm': '기타',\n",
        "       '친환경 접시 23cm': '기타',\n",
        "       '카스 병(단체)': '주류',\n",
        "       '콜라 (단체)': '음료',\n",
        "       '햇반': '추가메뉴',\n",
        "       '허브솔트': '추가메뉴',\n",
        "       '(단체) 공깃밥': '추가메뉴',\n",
        "       '(단체) 생목살 김치전골 2.0': '메인메뉴',\n",
        "       '(단체) 은이버섯 갈비탕': '메인메뉴',\n",
        "       '(단체) 한우 우거지 국밥': '메인메뉴',\n",
        "       '(단체) 황태해장국 3/27까지': '메인메뉴',\n",
        "       '(정식) 된장찌개': '메인메뉴',\n",
        "       '(정식) 물냉면 ': '메인메뉴',\n",
        "       '(정식) 비빔냉면': '메인메뉴',\n",
        "       '(후식) 된장찌개': '추가메뉴',\n",
        "       '(후식) 물냉면': '추가메뉴',\n",
        "       '(후식) 비빔냉면': '추가메뉴',\n",
        "       '갑오징어 비빔밥': '메인메뉴',\n",
        "       '갱시기': '메인메뉴',\n",
        "       '공깃밥': '추가메뉴',\n",
        "       '꼬막 비빔밥': '메인메뉴',\n",
        "       '느린마을 막걸리': '주류',\n",
        "       '담하 한우 불고기': '메인메뉴',\n",
        "       '담하 한우 불고기 정식': '메인메뉴',\n",
        "       '더덕 한우 지짐': '메인메뉴',\n",
        "       '들깨 양지탕': '메인메뉴',\n",
        "       '라면사리': '추가메뉴',\n",
        "       '룸 이용료': '기타',\n",
        "       '메밀면 사리': '추가메뉴',\n",
        "       '명인안동소주': '주류',\n",
        "       '명태회 비빔냉면': '메인메뉴',\n",
        "       '문막 복분자 칵테일': '주류',\n",
        "       '봉평메밀 물냉면': '메인메뉴',\n",
        "       '생목살 김치찌개': '메인메뉴',\n",
        "       '스프라이트': '음료',\n",
        "       '은이버섯 갈비탕': '메인메뉴',\n",
        "       '제로콜라': '음료',\n",
        "       '참이슬': '주류',\n",
        "       '처음처럼': '주류',\n",
        "       '카스': '주류',\n",
        "       '콜라': '음료',\n",
        "       '테라': '주류',\n",
        "       '하동 매실 칵테일': '주류',\n",
        "       '한우 떡갈비 정식': '메인메뉴',\n",
        "       '한우 미역국 정식': '메인메뉴',\n",
        "       '한우 우거지 국밥': '메인메뉴',\n",
        "       '한우 차돌박이 된장찌개': '메인메뉴',\n",
        "       '황태해장국': '메인메뉴',\n",
        "       'AUS (200g)': '메인메뉴',\n",
        "       'G-Charge(3)': '기타',\n",
        "       'Gls.Sileni': '주류',\n",
        "       'Gls.미션 서드': '주류',\n",
        "       'Open Food': '기타',\n",
        "       '그릴드 비프 샐러드': '메인메뉴',\n",
        "       '까르보나라': '메인메뉴',\n",
        "       '모둠 해산물 플래터': '메인메뉴',\n",
        "       '미션 서드 카베르네 쉬라': '메인메뉴',\n",
        "       '버섯 크림 리조또': '메인메뉴',\n",
        "       '빵 추가 (1인)': '추가메뉴',\n",
        "       '시저 샐러드 ': '메인메뉴',\n",
        "       '아메리카노': '음료',\n",
        "       '알리오 에 올리오 ': '메인메뉴',\n",
        "       '양갈비 (4ps)': '메인메뉴',\n",
        "       '자몽리치에이드': '음료',\n",
        "       '하이네켄(생)': '주류',\n",
        "       '한우 (200g)': '메인메뉴',\n",
        "       '해산물 토마토 리조또': '메인메뉴',\n",
        "       '해산물 토마토 스튜 파스타': '메인메뉴',\n",
        "       '해산물 토마토 스파게티': '메인메뉴',\n",
        "       '(단체)브런치주중 36,000': '메인메뉴',\n",
        "       '(오븐) 하와이안 쉬림프 피자': '메인메뉴',\n",
        "       '(화덕) 불고기 페퍼로니 반반피자': '메인메뉴',\n",
        "       'BBQ Platter': '메인메뉴',\n",
        "       'BBQ 고기추가': '추가메뉴',\n",
        "       '글라스와인 (레드)': '주류',\n",
        "       '레인보우칵테일(알코올)': '주류',\n",
        "       '미라시아 브런치 (패키지)': '메인메뉴',\n",
        "       '버드와이저(무제한)': '주류',\n",
        "       '보일링 랍스타 플래터': '메인메뉴',\n",
        "       '보일링 랍스타 플래터(덜매운맛)': '메인메뉴',\n",
        "       '브런치 2인 패키지 ': '메인메뉴',\n",
        "       '브런치 4인 패키지 ': '메인메뉴',\n",
        "       '브런치(대인) 주말': '메인메뉴',\n",
        "       '브런치(대인) 주중': '메인메뉴',\n",
        "       '브런치(어린이)': '메인메뉴',\n",
        "       '쉬림프 투움바 파스타': '메인메뉴',\n",
        "       '스텔라(무제한)': '주류',\n",
        "       '애플망고 에이드': '음료',\n",
        "       '얼그레이 하이볼': '주류',\n",
        "       '오븐구이 윙과 킬바사소세지': '메인메뉴',\n",
        "       '유자 하이볼': '주류',\n",
        "       '잭 애플 토닉': '주류',\n",
        "       '칠리 치즈 프라이': '추가메뉴',\n",
        "       '코카콜라': '음료',\n",
        "       '코카콜라(제로)': '음료',\n",
        "       '콥 샐러드': '추가메뉴',\n",
        "       '파스타면 추가(150g)': '추가메뉴',\n",
        "       '핑크레몬에이드': '음료',\n",
        "       'Cass Beer': '주류',\n",
        "       'Conference L1': '연회장 대여',\n",
        "       'Conference L2': '연회장 대여',\n",
        "       'Conference L3': '연회장 대여',\n",
        "       'Conference M1': '연회장 대여',\n",
        "       'Conference M8': '연회장 대여',\n",
        "       'Conference M9': '연회장 대여',\n",
        "       'Convention Hall': '연회장 대여',\n",
        "       'Cookie Platter': '디저트',\n",
        "       'Grand Ballroom': '연회장 대여',\n",
        "       'OPUS 2': '연회장 대여',\n",
        "       'Regular Coffee': '음료',\n",
        "       '골뱅이무침': '메인메뉴',\n",
        "       '돈목살 김치찌개 (밥포함)': '메인메뉴',\n",
        "       '로제 치즈떡볶이': '메인메뉴',\n",
        "       '마라샹궈': '메인메뉴',\n",
        "       '매콤 무뼈닭발&계란찜': '메인메뉴',\n",
        "       '모둠 돈육구이(3인)': '메인메뉴',\n",
        "       '삼겹살추가 (200g)': '추가메뉴',\n",
        "       '야채추가': '추가메뉴',\n",
        "       '왕갈비치킨': '메인메뉴',\n",
        "       '주먹밥 (2ea)': '추가메뉴',\n",
        "       '공깃밥(추가)': '추가메뉴',\n",
        "       '구슬아이스크림': '디저트',\n",
        "       '단체식 13000(신)': '메인메뉴',\n",
        "       '단체식 18000(신)': '메인메뉴',\n",
        "       '돼지고기 김치찌개': '메인메뉴',\n",
        "       '복숭아 아이스티': '음료',\n",
        "       '새우 볶음밥': '메인메뉴',\n",
        "       '새우튀김 우동': '메인메뉴',\n",
        "       '샷 추가': '추가메뉴',\n",
        "       '수제 등심 돈까스': '메인메뉴',\n",
        "       '아메리카노(HOT)': '음료',\n",
        "       '아메리카노(ICE)': '음료',\n",
        "       '약 고추장 돌솥비빔밥': '메인메뉴',\n",
        "       '어린이 돈까스': '메인메뉴',\n",
        "       '오픈푸드': '기타',\n",
        "       '진사골 설렁탕': '메인메뉴',\n",
        "       '짜장면': '메인메뉴',\n",
        "       '짜장밥': '메인메뉴',\n",
        "       '짬뽕': '메인메뉴',\n",
        "       '짬뽕밥': '메인메뉴',\n",
        "       '치즈돈까스': '메인메뉴',\n",
        "       '카페라떼(HOT)': '음료',\n",
        "       '카페라떼(ICE)': '음료',\n",
        "       '한상 삼겹구이 정식(2인) 소요시간 약 15~20분': '메인메뉴',\n",
        "       '꼬치어묵': '메인메뉴',\n",
        "       '떡볶이': '메인메뉴',\n",
        "       '생수': '음료',\n",
        "       '치즈 핫도그': '디저트',\n",
        "       '페스츄리 소시지': '디저트',\n",
        "       '단호박 식혜 ': '음료',\n",
        "       '병천순대': '메인메뉴',\n",
        "       '참살이 막걸리': '주류',\n",
        "       '찹쌀식혜': '음료',\n",
        "       '해물파전': '메인메뉴',\n",
        "       '메밀미숫가루': '음료',\n",
        "       '아메리카노 HOT': '음료',\n",
        "       '아메리카노 ICE': '음료',\n",
        "       '카페라떼 ICE': '음료',\n",
        "       '현미뻥스크림': '디저트'\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 697,
      "metadata": {
        "id": "-hKCqzlYDOKZ"
      },
      "outputs": [],
      "source": [
        "# Assign the menu_category to df\n",
        "train['menu_category'] = train['메뉴명'].map(menu_category)\n",
        "train['menu_category'], uniques = pd.factorize(train['menu_category'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# avg_sales_all_days"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 698,
      "metadata": {},
      "outputs": [],
      "source": [
        "menu_avg = train.groupby('메뉴명')['매출수량'].mean()\n",
        "train['avg_sales_all_days'] = train['메뉴명'].map(menu_avg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature about zero\n",
        "1. avg_sales_nonzero_days\n",
        "2. zero_sales_day_ratio\n",
        "3. is_sparse_menu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 699,
      "metadata": {
        "id": "x_3VFMgYDRpE"
      },
      "outputs": [],
      "source": [
        "nonzero_avg = train[train['매출수량'] > 0].groupby('메뉴명')['매출수량'].mean()\n",
        "# Assign the nonzero_avg to df\n",
        "train['avg_sales_nonzero_days'] = train['메뉴명'].map(nonzero_avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 700,
      "metadata": {
        "id": "6-VFqzBjDWw6"
      },
      "outputs": [],
      "source": [
        "zero_ratio = train.groupby('메뉴명')['매출수량'].apply(lambda x: (x.eq(0).sum() / len(x)) * 100)\n",
        "\n",
        "# Assign the zero_ratio to df\n",
        "train['zero_sales_day_ratio'] = train['메뉴명'].map(zero_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 701,
      "metadata": {},
      "outputs": [],
      "source": [
        "train['is_sparse_menu'] = np.where(train['zero_sales_day_ratio'] > 50, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature about 연회장\n",
        "1. banquet_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 702,
      "metadata": {
        "id": "-ZJ2qJ_WD9Hu"
      },
      "outputs": [],
      "source": [
        "df_연회장 = train[train['영업장명']=='연회장'].pivot_table(index='영업일자',columns='메뉴명',values='매출수량', aggfunc = 'sum').reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 703,
      "metadata": {
        "id": "FFvFvhtdD_lR"
      },
      "outputs": [],
      "source": [
        "df_연회장['연회장 대여'] = df_연회장[['Conference L1','Conference L2','Conference L3','Conference M1','Conference M8','Conference M9','Convention Hall','Grand Ballroom','OPUS 2']].sum(axis=1)\n",
        "df_연회장['음료 및 쿠키'] = df_연회장[['Cookie Platter','Cass Beer','Regular Coffee']].sum(axis=1)\n",
        "df_연회장['음식'] = df_연회장[['골뱅이무침','공깃밥','돈목살 김치찌개 (밥포함)','로제 치즈떡볶이','마라샹궈','매콤 무뼈닭발&계란찜','모둠 돈육구이(3인)','삼겹살추가 (200g)','야채추가','왕갈비치킨','주먹밥 (2ea)']].sum(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 704,
      "metadata": {
        "id": "othgXoAtEDKu"
      },
      "outputs": [],
      "source": [
        "def banquet_type(row):\n",
        "    if row['연회장 대여'] > 0 and row['음식'] == 0 and row['음료 및 쿠키'] == 0:\n",
        "        return 1 # 대여만\n",
        "    elif row['연회장 대여'] == 0 and row['음식'] == 0 and row['음료 및 쿠키'] > 0:\n",
        "        return 2 # 음료및쿠키만\n",
        "    elif row['연회장 대여'] == 0 and row['음식'] > 0 and row['음료 및 쿠키'] == 0:\n",
        "        return 3 # 음식만\n",
        "    elif row['연회장 대여'] > 0 and row['음식'] == 0 and row['음료 및 쿠키'] > 0:\n",
        "        return 4 # 대여+음료및쿠키\n",
        "    elif row['연회장 대여'] > 0 and row['음식'] > 0 and row['음료 및 쿠키'] == 0:\n",
        "        return 5 # 대여+음식\n",
        "    elif row['연회장 대여'] == 0 and row['음식'] > 0 and row['음료 및 쿠키'] > 0:\n",
        "        return 6 # 음식+음료및쿠키\n",
        "    elif row['연회장 대여'] > 0 and row['음식'] > 0 and row['음료 및 쿠키'] > 0:\n",
        "        return 7 # 대여+음료및쿠키+음식\n",
        "    else:\n",
        "        return 0 # 연회장 총매출이 0인경우\n",
        "df_연회장['banquet_type'] = df_연회장.apply(banquet_type, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Features\n",
        "1. is_drink\n",
        "2. is_alcohol\n",
        "3. is_set_menu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 705,
      "metadata": {
        "id": "dJnoaw68EKgv"
      },
      "outputs": [],
      "source": [
        "drink_keywords = ['콜라', '스프라이트', '제로콜라', '자몽리치에이드', '애플망고 에이드', '핑크레몬에이드', '아메리카노',\n",
        "                  '식혜', '메밀미숫가루', '아메리카노', '카페라떼', '복숭아 아이스티','샷 추가',\n",
        "                  '생수']\n",
        "\n",
        "alcohol_keywords = ['Gls.Sileni', 'Gls.미션 서드', '미션 서드 카메르네 쉬라', '하이네켄', '막걸리',\n",
        "                    '와인', '버드와이저', '스텔라', '하이볼', '잭 애플 토닉', '참이슬', '소주', '처음처럼',\n",
        "                    '카스', '테라', '칵테일', 'Cass']\n",
        "\n",
        "set_keywords = ['정식']\n",
        "\n",
        "train['is_drink'] = train['영업장명_메뉴명'].apply(\n",
        "    lambda x: 1 if any(keyword in str(x) for keyword in drink_keywords) else 0\n",
        ")\n",
        "\n",
        "train['is_alcohol'] = train['영업장명_메뉴명'].apply(\n",
        "    lambda x: 1 if (\n",
        "        any(keyword in str(x) for keyword in alcohol_keywords)\n",
        "        and '컵' not in str(x)\n",
        "    ) else 0\n",
        ")\n",
        "\n",
        "train['is_set_menu'] = train['영업장명_메뉴명'].apply(\n",
        "    lambda x: 1 if (\n",
        "        any(keyword in str(x) for keyword in set_keywords)\n",
        "    ) else 0\n",
        ")\n",
        "\n",
        "#매출수량이 문자열이면 숫자로 변환\n",
        "train['매출수량'] = pd.to_numeric(train['매출수량'], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# demand_volatility, demand_stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 706,
      "metadata": {
        "id": "7cBAAeC_ELIC"
      },
      "outputs": [],
      "source": [
        "menu_stats=(\n",
        "    train.groupby('영업장명_메뉴명')['매출수량']\n",
        "    .agg(['mean','std'])\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "menu_stats['demand_volatility']=menu_stats['std']/menu_stats['mean']\n",
        "menu_stats['demand_stability']=1/menu_stats['demand_volatility']\n",
        "\n",
        "menu_stats.rename(columns={'mean':'평균매출수량','std':'표준편차'},inplace=True)\n",
        "\n",
        "# Merge menu_stats back to df\n",
        "train=train.merge(menu_stats[['영업장명_메뉴명', 'demand_volatility','demand_stability']], on='영업장명_메뉴명',how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add New"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 707,
      "metadata": {},
      "outputs": [],
      "source": [
        "train['is_nonzero'] = (train['매출수량'] > 0).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXQJy9_mE19e"
      },
      "source": [
        "# **데이터 전처리**\n",
        "\n",
        "1. **is_spike**: 전식당, 변할 수 있는 가능성을 학습하게끔 하는 것, 당일 수치가 최근 7일간 평균 + 2 × 표준편차보다 크면 1, 아니면 0, 위로 갑자기 튀는 것을 포착\n",
        "2. **is_drop**: 전식당, 변할 수 있는 가능성을 학습하게끔 하는 것, 당일 수치가 최근 7일간 평균 − 2 × 표준편차보다 작으면 1, 아니면 0, 아래로 갑자기 튀는 것을 포착\n",
        "3. **is_weekday_price**: 미라시아, 요금제가 주중 기준인지 여부 구분\t메뉴명에 '주중'이 포함되면 1, 아니면 0, 주중 요금이 적용된 메뉴인지 여부\n",
        "4. **is_weekend_price**: 미라시아, 요금제가 주말 기준인지 여부 구분\t메뉴명에 '주말'이 포함되면 1, 아니면 0, 주말 요금이 적용된 메뉴인지 여부\n",
        "5. **seasonal_index**: 전식당,\n",
        "\n",
        "    • 1분기 (Q1): 1월 1일부터 3월 31일까지\n",
        "\n",
        "    • 2분기 (Q2): 4월 1일부터 6월 30일까지\n",
        "\n",
        "    • 3분기 (Q3): 7월 1일부터 9월 30일까지\n",
        "\n",
        "    • 4분기 (Q4): 10월 1일부터 12월 31일까지\n",
        "\n",
        "    월별 또는 분기별 매출 패턴 분석하여 생성, 분기별 매출 수치화\n",
        "6. **미라시아 단체 관련 변수( brunch_flag, hallroom_flag)** :\n",
        "    \n",
        "    6-1. **brunch_flag**: 단체 브런치 메뉴 매출이 생긴 날의 플래그, 연회장_룸타입에만 플래그를 세운다.\n",
        "\n",
        "    6-2. **hallroom_flag**: 연회장_룸타입 매출이 생긴 날의 플래그, (단체)브런치주중 36,000 에만 플래그를 세운다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCZt3VuGI5Vr"
      },
      "source": [
        "# quarter_index(all)\n",
        "\n",
        "생성 목적: 분기별 매출 수치화\n",
        "\n",
        "생성 방법:\n",
        "\n",
        "• 1분기 (Q1): 1월 1일부터 3월 31일까지\n",
        "\n",
        "• 2분기 (Q2): 4월 1일부터 6월 30일까지\n",
        "\n",
        "• 3분기 (Q3): 7월 1일부터 9월 30일까지\n",
        "\n",
        "• 4분기 (Q4): 10월 1일부터 12월 31일까지\n",
        "\n",
        "분기별 매출 패턴 분석하여 누적합으로 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 708,
      "metadata": {
        "id": "WT7YEnSZDb7f"
      },
      "outputs": [],
      "source": [
        "# # 0) 날짜 보정\n",
        "# if df['영업일자'].dtype == 'O':\n",
        "#     df['영업일자'] = pd.to_datetime(df['영업일자'], errors='coerce')\n",
        "\n",
        "# 1) '분기' → 'quarter'로 변경 (없으면 새로 생성)\n",
        "#     Q1, Q2, Q3, Q4 형식으로 생성\n",
        "train['quarter'] = train['영업일자'].dt.to_period('Q').astype(str).str[-2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 709,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_time_feats(df: pd.DataFrame, date_col: str = '영업일자') -> pd.DataFrame:\n",
        "    # 사본에서 계산 (원본 보존)\n",
        "    df = df.copy()\n",
        "    # 날짜 캐스팅\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "\n",
        "    # 기본 파생\n",
        "    year  = df[date_col].dt.year.astype('Int16')\n",
        "    month = df[date_col].dt.month.astype('Int8')\n",
        "    day   = df[date_col].dt.day.astype('Int8')\n",
        "\n",
        "    # 분기/분기 내 진행일\n",
        "    q = df[date_col].dt.to_period('Q')\n",
        "    q_start = q.dt.start_time\n",
        "    quarter = q.astype(str).str[-2:].map({'Q1':0,'Q2':1,'Q3':2,'Q4':3}).astype('Int8')\n",
        "    day_of_quarter = (df[date_col] - q_start).dt.days.astype('Int16')\n",
        "\n",
        "    # 결과 합치기\n",
        "    df['year'] = year\n",
        "    df['month'] = month\n",
        "    df['day'] = day\n",
        "    df['quarter'] = quarter\n",
        "    df['day_of_quarter'] = day_of_quarter\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 710,
      "metadata": {
        "id": "1x3cWKtxL_wz"
      },
      "outputs": [],
      "source": [
        "# 0) 시간 피처\n",
        "train = add_time_feats(train, '영업일자')  # 이미 정의된 함수 사용\n",
        "train = train.sort_values(['영업장명_메뉴명', '영업일자']).copy()\n",
        "\n",
        "# 1) 분기 내 누적 비율\n",
        "grp = ['영업장명_메뉴명', 'year', 'quarter']\n",
        "train['quarter_cum'] = train.groupby(grp)['매출수량'].cumsum()\n",
        "train['quarter_tot'] = train.groupby(grp)['매출수량'].transform('sum')\n",
        "train['cum_share_actual'] = (train['quarter_cum'] / train['quarter_tot']).astype('float32')\n",
        "\n",
        "# 2) 참조 곡선: (매장메뉴, day_of_quarter) 중앙값\n",
        "curve_ref = (\n",
        "    train.groupby(['영업장명_메뉴명','day_of_quarter'])['cum_share_actual']\n",
        "         .median()\n",
        "         .reset_index()\n",
        "         .rename(columns={'cum_share_actual':'cum_share_ref_doq'})\n",
        ")\n",
        "curve_ref['day_of_quarter'] = curve_ref['day_of_quarter'].astype('Int16')\n",
        "\n",
        "# 2-보강) 전역 백업값\n",
        "global_md_doq = (\n",
        "    train.groupby(['day_of_quarter'])['cum_share_actual']\n",
        "         .median()\n",
        "         .reset_index()\n",
        "         .rename(columns={'cum_share_actual':'cum_share_doq_md'})\n",
        ")\n",
        "global_md_doq['day_of_quarter'] = global_md_doq['day_of_quarter'].astype('Int16')\n",
        "\n",
        "global_md_month_day = (\n",
        "    train.groupby(['month','day'])['cum_share_actual']\n",
        "         .median()\n",
        "         .reset_index()\n",
        "         .rename(columns={'cum_share_actual':'cum_share_global_md'})\n",
        ")\n",
        "global_md_month_day['month'] = global_md_month_day['month'].astype('Int8')\n",
        "global_md_month_day['day']   = global_md_month_day['day'].astype('Int8')\n",
        "\n",
        "# 키 dtype 정리\n",
        "train['month']          = train['month'].astype('Int8')\n",
        "train['day']            = train['day'].astype('Int8')\n",
        "train['day_of_quarter'] = train['day_of_quarter'].astype('Int16')\n",
        "\n",
        "# 기존 cum_share_ref 제거\n",
        "train = train.drop(columns=['cum_share_ref'], errors='ignore')\n",
        "\n",
        "# 3) 매핑: 우선 doq → 전역 doq → (month,day) → 0.5\n",
        "train = train.merge(\n",
        "    curve_ref, on=['영업장명_메뉴명','day_of_quarter'],\n",
        "    how='left', validate='many_to_one'\n",
        ")\n",
        "train = train.merge(\n",
        "    global_md_doq, on='day_of_quarter',\n",
        "    how='left', validate='many_to_one'\n",
        ")\n",
        "train = train.merge(\n",
        "    global_md_month_day, on=['month','day'],\n",
        "    how='left', validate='many_to_one'\n",
        ")\n",
        "\n",
        "train['cum_share_ref'] = (\n",
        "    train['cum_share_ref_doq']\n",
        "        .fillna(train['cum_share_doq_md'])\n",
        "        .fillna(train['cum_share_global_md'])\n",
        "        .fillna(0.5)\n",
        ").astype('float32')\n",
        "\n",
        "# 4) (강력 추천) 분기 내 단조(비감소) 보정\n",
        "train = train.sort_values(['영업장명_메뉴명','year','quarter','day_of_quarter'])\n",
        "train['cum_share_ref'] = (\n",
        "    train.groupby(['영업장명_메뉴명','year','quarter'])['cum_share_ref']\n",
        "         .apply(lambda s: s.ffill().bfill().cummax().clip(0,1))\n",
        "         .reset_index(level=[0,1,2], drop=True)\n",
        ")\n",
        "\n",
        "# 5) 임시 컬럼/중간 계산 정리\n",
        "train = train.drop(\n",
        "    columns=[\n",
        "        'cum_share_ref_doq','cum_share_doq_md','cum_share_global_md',\n",
        "        'quarter_cum','quarter_tot','cum_share_actual'\n",
        "    ],\n",
        "    errors='ignore'\n",
        ")\n",
        "# 필요시 day_of_quarter도 정리(남겨두고 싶으면 주석)\n",
        "# train = train.drop(columns=['day_of_quarter'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# seasonal_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 711,
      "metadata": {},
      "outputs": [],
      "source": [
        "# month -> season (0:겨울, 1:봄, 2:여름, 3:가을)\n",
        "def get_season_code(month: int) -> int:\n",
        "    if month in (12, 1, 2):   return 0\n",
        "    if month in (3, 4, 5):    return 1\n",
        "    if month in (6, 7, 8):    return 2\n",
        "    return 3  # 9~11\n",
        "\n",
        "def add_season_feats(df: pd.DataFrame, date_col: str = '영업일자') -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    out[date_col] = pd.to_datetime(out[date_col], errors='coerce')\n",
        "\n",
        "    # 기본 날짜 파생 (NumPy 정수형으로 맞춤)\n",
        "    out['year']  = out[date_col].dt.year.astype('int16')\n",
        "    out['month'] = out[date_col].dt.month.astype('int8')\n",
        "    out['day']   = out[date_col].dt.day.astype('int8')\n",
        "\n",
        "    # season (0:겨울, 1:봄, 2:여름, 3:가을)\n",
        "    out['season'] = out['month'].apply(get_season_code).astype('int8')\n",
        "\n",
        "    # 시즌 시작 연도(season_year) 계산\n",
        "    y = out['year'].astype('int32')\n",
        "    m = out['month'].astype('int32')\n",
        "    season_year = y.copy()\n",
        "    season_year[(out['season'] == 0) & (m <= 2)] -= 1  # 겨울의 1~2월은 전년도\n",
        "    out['season_year'] = season_year.astype('int16')\n",
        "\n",
        "    # 시즌 시작 월(start_month) → np.select 결과는 ndarray이므로 'int8' 사용\n",
        "    start_month = np.select(\n",
        "        [\n",
        "            out['season'].eq(0),  # 겨울\n",
        "            out['season'].eq(1),  # 봄\n",
        "            out['season'].eq(2),  # 여름\n",
        "        ],\n",
        "        [12, 3, 6],\n",
        "        default=9               # 가을\n",
        "    ).astype('int8')            # ★ 여기! 'Int8'이 아니라 'int8'\n",
        "\n",
        "    # 시즌 시작일\n",
        "    season_start = pd.to_datetime({\n",
        "        \"year\":  out['season_year'].astype(int),  # to_datetime에는 일반 int가 안전\n",
        "        \"month\": start_month.astype(int),\n",
        "        \"day\":   1\n",
        "    })\n",
        "\n",
        "    # 시즌 내 경과일\n",
        "    out['day_of_season'] = (out[date_col] - season_start).dt.days.astype('int16')\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 712,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 0) 시즌 피처 생성 + 정렬\n",
        "train_s = add_season_feats(train, '영업일자')\n",
        "train_s = train_s.sort_values(['영업장명_메뉴명', '영업일자']).copy()\n",
        "\n",
        "# 1) 시즌 내 누적 비율 (누적/시즌합)\n",
        "grp_season = ['영업장명_메뉴명', 'season_year', 'season']\n",
        "train_s['season_cum'] = train_s.groupby(grp_season)['매출수량'].cumsum()\n",
        "train_s['season_tot'] = train_s.groupby(grp_season)['매출수량'].transform('sum')\n",
        "train_s['cum_share_actual_season'] = (train_s['season_cum'] / train_s['season_tot']).astype('float32')\n",
        "\n",
        "# 2) 참조 곡선: (매장메뉴, day_of_season) 중앙값 (로버스트)\n",
        "curve_ref_season = (\n",
        "    train_s.groupby(['영업장명_메뉴명', 'day_of_season'])['cum_share_actual_season']\n",
        "           .median()\n",
        "           .reset_index()\n",
        "           .rename(columns={'cum_share_actual_season': 'cum_share_ref_dos'})\n",
        ")\n",
        "curve_ref_season['day_of_season'] = curve_ref_season['day_of_season'].astype('Int16')\n",
        "\n",
        "# 2-보강) 전역 백오프\n",
        "global_md_dos = (\n",
        "    train_s.groupby(['day_of_season'])['cum_share_actual_season']\n",
        "           .median()\n",
        "           .reset_index()\n",
        "           .rename(columns={'cum_share_actual_season': 'cum_share_dos_md'})\n",
        ")\n",
        "global_md_dos['day_of_season'] = global_md_dos['day_of_season'].astype('Int16')\n",
        "\n",
        "global_md_month_day = (\n",
        "    train_s.groupby(['month','day'])['cum_share_actual_season']\n",
        "           .median()\n",
        "           .reset_index()\n",
        "           .rename(columns={'cum_share_actual_season': 'cum_share_global_md'})\n",
        ")\n",
        "global_md_month_day['month'] = global_md_month_day['month'].astype('Int8')\n",
        "global_md_month_day['day']   = global_md_month_day['day'].astype('Int8')\n",
        "\n",
        "# 3) train에 시즌 참조 붙이기: dos → dos_global → (month,day) → 0.5\n",
        "train_s = train_s.merge(\n",
        "    curve_ref_season, on=['영업장명_메뉴명','day_of_season'],\n",
        "    how='left', validate='many_to_one'\n",
        ").merge(\n",
        "    global_md_dos, on='day_of_season',\n",
        "    how='left', validate='many_to_one'\n",
        ").merge(\n",
        "    global_md_month_day, on=['month','day'],\n",
        "    how='left', validate='many_to_one'\n",
        ")\n",
        "\n",
        "train_s['cum_share_ref_season'] = (\n",
        "    train_s['cum_share_ref_dos']\n",
        "          .fillna(train_s['cum_share_dos_md'])\n",
        "          .fillna(train_s['cum_share_global_md'])\n",
        "          .fillna(0.5)\n",
        ").astype('float32')\n",
        "\n",
        "# 4) (강추) 시즌 내 단조(비감소) 보정\n",
        "train_s = train_s.sort_values(['영업장명_메뉴명','season_year','season','day_of_season'])\n",
        "train_s['cum_share_ref_season'] = (\n",
        "    train_s.groupby(['영업장명_메뉴명','season_year','season'])['cum_share_ref_season']\n",
        "           .apply(lambda s: s.ffill().bfill().cummax().clip(0,1))\n",
        "           .reset_index(level=[0,1,2], drop=True)\n",
        ")\n",
        "\n",
        "# 5) 원본 train에 최종 칼럼만 붙이기(원본 순서 보존)\n",
        "train = train.copy()\n",
        "train['cum_share_ref_season'] = train_s.sort_index()['cum_share_ref_season'].values\n",
        "train['season'] = train_s.sort_index()['season'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# solar_term_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 713,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 0) 날짜형 통일 ---\n",
        "train['영업일자'] = pd.to_datetime(train['영업일자'], errors='coerce')\n",
        "terms['locdate']  = pd.to_datetime(terms['locdate'],  errors='coerce')\n",
        "\n",
        "# --- 1) 절기 구간 정의 (start/end) ---\n",
        "terms = terms.sort_values('locdate').reset_index(drop=True)\n",
        "terms['end_date'] = terms['locdate'].shift(-1) - pd.Timedelta(days=1)\n",
        "terms.loc[terms.index[-1], 'end_date'] = train['영업일자'].max()\n",
        "\n",
        "# --- 2) asof 머지로 절기 매핑 ---\n",
        "# 왼쪽에 solar_term 있으면 충돌 방지 위해 제거\n",
        "left  = train.drop(columns=['solar_term'], errors='ignore').sort_values('영업일자')\n",
        "right = terms[['locdate', 'end_date', 'solar_term']].sort_values('locdate')\n",
        "\n",
        "merged = pd.merge_asof(\n",
        "    left, right,\n",
        "    left_on='영업일자', right_on='locdate',\n",
        "    direction='backward',\n",
        "    # suffixes=('', '_r')  # (옵션) 충돌 시 오른쪽 접미사\n",
        ")\n",
        "\n",
        "# 구간 내만 유효\n",
        "in_range = merged['영업일자'] <= merged['end_date']\n",
        "# 이제 merged에는 'solar_term'이 반드시 존재\n",
        "merged['solar_term'] = merged['solar_term'].where(in_range)\n",
        "\n",
        "# --- 3) term_cycle_id & day_of_term 산출 ---\n",
        "train_st = merged.rename(columns={'locdate':'term_start', 'end_date':'term_end'}).copy()\n",
        "train_st['term_start'] = pd.to_datetime(train_st['term_start'])\n",
        "train_st['term_end']   = pd.to_datetime(train_st['term_end'])\n",
        "train_st['day_of_term'] = (train_st['영업일자'] - train_st['term_start']).dt.days.astype('Int16')\n",
        "train_st['term_cycle_id'] = train_st['term_start']  # 각 절기 사이클 식별자\n",
        "\n",
        "# 이하 동일 (누적/참조/백오프/단조보정/최종 부착)\n",
        "\n",
        "# --- 4) 절기 내 누적비율(actual) 계산 ---\n",
        "grp_term = ['영업장명_메뉴명', 'solar_term', 'term_cycle_id']\n",
        "train_st = train_st.sort_values(['영업장명_메뉴명','영업일자'])\n",
        "train_st['term_cum'] = train_st.groupby(grp_term)['매출수량'].cumsum()\n",
        "train_st['term_tot'] = train_st.groupby(grp_term)['매출수량'].transform('sum')\n",
        "train_st['cum_share_actual_term'] = (train_st['term_cum'] / train_st['term_tot']).astype('float32')\n",
        "\n",
        "# --- 5) 참조곡선: (매장메뉴, solar_term, day_of_term) 중앙값 ---\n",
        "curve_ref_term = (\n",
        "    train_st.groupby(['영업장명_메뉴명','solar_term','day_of_term'])['cum_share_actual_term']\n",
        "            .median()\n",
        "            .reset_index()\n",
        "            .rename(columns={'cum_share_actual_term':'cum_share_ref_dot'})  # dot = day_of_term\n",
        ")\n",
        "curve_ref_term['day_of_term'] = curve_ref_term['day_of_term'].astype('Int16')\n",
        "\n",
        "# --- 6) 전역 백오프 ---\n",
        "# (1) 같은 solar_term 내 day_of_term 중앙값\n",
        "global_md_dot = (\n",
        "    train_st.groupby(['solar_term','day_of_term'])['cum_share_actual_term']\n",
        "            .median()\n",
        "            .reset_index()\n",
        "            .rename(columns={'cum_share_actual_term':'cum_share_term_dot_md'})\n",
        ")\n",
        "global_md_dot['day_of_term'] = global_md_dot['day_of_term'].astype('Int16')\n",
        "\n",
        "# (2) 마지막 백업: (month, day) 캘린더 중앙값\n",
        "train_st['month'] = train_st['영업일자'].dt.month.astype('Int8')\n",
        "train_st['day']   = train_st['영업일자'].dt.day.astype('Int8')\n",
        "global_md_month_day_term = (\n",
        "    train_st.groupby(['month','day'])['cum_share_actual_term']\n",
        "            .median()\n",
        "            .reset_index()\n",
        "            .rename(columns={'cum_share_actual_term':'cum_share_global_md_term'})\n",
        ")\n",
        "\n",
        "# --- 7) 참조 매핑: dot → term-dot-global → (month,day) → 0.5 ---\n",
        "train_st = train_st.merge(\n",
        "    curve_ref_term, on=['영업장명_메뉴명','solar_term','day_of_term'],\n",
        "    how='left', validate='many_to_one'\n",
        ").merge(\n",
        "    global_md_dot, on=['solar_term','day_of_term'],\n",
        "    how='left', validate='many_to_one'\n",
        ").merge(\n",
        "    global_md_month_day_term, on=['month','day'],\n",
        "    how='left', validate='many_to_one'\n",
        ")\n",
        "\n",
        "train_st['cum_share_ref_solar'] = (\n",
        "    train_st['cum_share_ref_dot']\n",
        "           .fillna(train_st['cum_share_term_dot_md'])\n",
        "           .fillna(train_st['cum_share_global_md_term'])\n",
        "           .fillna(0.5)\n",
        ").astype('float32')\n",
        "\n",
        "# --- 8) (권장) 절기 내 단조(비감소) 보정 ---\n",
        "train_st = train_st.sort_values(['영업장명_메뉴명','solar_term','term_cycle_id','day_of_term'])\n",
        "train_st['cum_share_ref_solar'] = (\n",
        "    train_st.groupby(['영업장명_메뉴명','solar_term','term_cycle_id'])['cum_share_ref_solar']\n",
        "            .apply(lambda s: s.ffill().bfill().cummax().clip(0,1))\n",
        "            .reset_index(level=[0,1,2], drop=True)\n",
        ")\n",
        "\n",
        "# --- 9) 원본 train에 최종 칼럼만 부착 & 보조컬럼 정리 ---\n",
        "train = train.copy()\n",
        "train['cum_share_ref_solar'] = train_st.sort_index()['cum_share_ref_solar'].values\n",
        "# 필요 없으면 보조 컬럼 정리(원본 train에는 없을 수 있으므로 ignore)\n",
        "train = train.drop(columns=['term_cycle_id','day_of_term'], errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 714,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['영업일자', '영업장명_메뉴명', '매출수량', 'time_idx', 'year', 'month', 'day', '영업장명',\n",
              "       '메뉴명', 'weekday', 'is_holiday', 'is_sandwich', 'is_before_holiday',\n",
              "       'is_after_holiday', 'is_weekend', 'is_popular', 'menu_category',\n",
              "       'avg_sales_all_days', 'avg_sales_nonzero_days', 'zero_sales_day_ratio',\n",
              "       'is_sparse_menu', 'is_drink', 'is_alcohol', 'is_set_menu',\n",
              "       'demand_volatility', 'demand_stability', 'is_nonzero', 'quarter',\n",
              "       'day_of_quarter', 'cum_share_ref', 'cum_share_ref_season', 'season',\n",
              "       'cum_share_ref_solar'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 714,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZezPQDTrKCN7"
      },
      "source": [
        "# **미라시아 단체 관련 변수( brunch_flag, hallroom_flag)**\n",
        "\n",
        "**brunch_flag**: 단체 브런치 메뉴 매출이 생긴 날의 플래그, 연회장_룸타입('Grand Ballroom', 'Convention Hall', 'Conference L', 'Conference M', 'OPUS 2')에만 플래그를 세운다.\n",
        "\n",
        "**hallroom_flag**: 연회장_룸타입 매출이 생긴 날의 플래그, (단체)브런치주중 36,000 에만 플래그를 세운다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 715,
      "metadata": {
        "id": "pCcPaPogV5La"
      },
      "outputs": [],
      "source": [
        "TARGET = '미라시아_(단체)브런치주중 36,000'\n",
        "HALL_ROOMS = {'Grand Ballroom', 'Convention Hall', 'Conference L', 'Conference M', 'OPUS'}\n",
        "\n",
        "# 날짜형 변환\n",
        "if train['영업일자'].dtype == 'O':\n",
        "    train['영업일자'] = pd.to_datetime(train['영업일자'], errors='coerce')\n",
        "\n",
        "# '영업장명_메뉴명' 분리\n",
        "tokens = train['영업장명_메뉴명'].str.split('_', n=1, expand=True)\n",
        "store0 = tokens[0].astype(str).str.strip()   # 예: '연회장', '미라시아', ...\n",
        "store1 = tokens[1].astype(str).str.strip()   # 예: 'Grand Ballroom', '(단체)브런치주중 36,000', ...\n",
        "\n",
        "# 1) 연회장 매출 발생 날짜\n",
        "hall_mask = (store0.eq('연회장')) & (store1.isin(HALL_ROOMS)) & (train['매출수량'] > 0)\n",
        "hall_dates = train.loc[hall_mask, '영업일자'].unique()\n",
        "\n",
        "# 2) 브런치 매출 발생 날짜\n",
        "brunch_mask = train['영업장명_메뉴명'].eq(TARGET) & (train['매출수량'] > 0)\n",
        "brunch_dates = train.loc[brunch_mask, '영업일자'].unique()\n",
        "\n",
        "# 3) 플래그 생성 (반대로 반영)\n",
        "# 초기화: 전부 0\n",
        "train['brunch_flag'] = 0      # ← 연회장 라인에 찍힘 (브런치 매출 발생일 기준)\n",
        "train['hallroom_flag'] = 0    # ← 미라시아 단체 브런치 라인에 찍힘 (연회장 매출 발생일 기준)\n",
        "\n",
        "# A) 단체 브런치 매출 발생일 → \"연회장_*\" 행에 brunch_flag=1\n",
        "train.loc[hall_mask & train['영업일자'].isin(brunch_dates), 'brunch_flag'] = 1\n",
        "\n",
        "# B) 연회장 매출 발생일 → \"미라시아_(단체)브런치주중 36,000\" 행에 hallroom_flag=1\n",
        "target_row_mask = train['영업장명_메뉴명'].eq(TARGET)\n",
        "train.loc[target_row_mask & train['영업일자'].isin(hall_dates), 'hallroom_flag'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 716,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0976ea6",
        "outputId": "0622eb8f-8dfa-4f66-d04b-8d7c2d48fe4b"
      },
      "outputs": [],
      "source": [
        "# Add features from the 'train' DataFrame to 'df'\n",
        "# Ensure '영업일자' is datetime in both dataframes for merging\n",
        "train['영업일자'] = pd.to_datetime(train['영업일자'], errors='coerce')\n",
        "\n",
        "# Merge based on '영업일자' and '영업장명_메뉴명'\n",
        "# Recreate '영업장명_메뉴명' in train for merging if it was dropped\n",
        "if '영업장명_메뉴명' not in train.columns:\n",
        "    train['영업장명_메뉴명'] = train['영업장명'] + '_' + train['메뉴명']\n",
        "\n",
        "# Add banquet_type from df_연회장\n",
        "# Ensure '영업일자' is datetime in df_연회장\n",
        "df_연회장['영업일자'] = pd.to_datetime(df_연회장['영업일자'], errors='coerce')\n",
        "\n",
        "train = pd.merge(train, df_연회장[['영업일자', 'banquet_type']], on='영업일자', how='left')\n",
        "\n",
        "# Fill NaN values in banquet_type with 0 (assuming 0 means no banquet)\n",
        "train['banquet_type'] = train['banquet_type'].fillna(0).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Store trian.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 717,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b926675",
        "outputId": "2704f493-21f1-4894-aa76-655867122d2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['영업일자', '영업장명_메뉴명', '매출수량', 'time_idx', 'year', 'month', 'day', '영업장명',\n",
            "       '메뉴명', 'weekday', 'is_holiday', 'is_sandwich', 'is_before_holiday',\n",
            "       'is_after_holiday', 'is_weekend', 'is_popular', 'menu_category',\n",
            "       'avg_sales_all_days', 'avg_sales_nonzero_days', 'zero_sales_day_ratio',\n",
            "       'is_sparse_menu', 'is_drink', 'is_alcohol', 'is_set_menu',\n",
            "       'demand_volatility', 'demand_stability', 'is_nonzero', 'quarter',\n",
            "       'day_of_quarter', 'cum_share_ref', 'cum_share_ref_season', 'season',\n",
            "       'cum_share_ref_solar', 'brunch_flag', 'hallroom_flag', 'banquet_type'],\n",
            "      dtype='object')\n",
            "train DataFrame이 output.csv로 저장되었습니다.\n"
          ]
        }
      ],
      "source": [
        "# train.drop(columns=['영업장명','메뉴명'], errors='ignore', inplace=True)\n",
        "train[['영업장명', '메뉴명']] = train['영업장명_메뉴명'].str.split('_', expand=True)\n",
        "# train.drop(columns=['day_of_quarter'], inplace=True, errors='ignore')\n",
        "# train.drop(columns=['cum_share_ref'], inplace=True, errors='ignore')\n",
        "# train DataFrame을 CSV로 저장 (모든 feature 포함)\n",
        "train.to_csv(\"re_train_05.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(train.columns)\n",
        "\n",
        "print(\"train DataFrame이 output.csv로 저장되었습니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bcdee13"
      },
      "source": [
        "# Task\n",
        "Apply the same feature engineering steps (date features, holiday features, spike/drop, seasonal index, banquet type, etc.) that were applied to the training data to the following test files: \"TEST_01.csv\", \"TEST_02.csv\", \"TEST_03.csv\", \"TEST_04.csv\", \"TEST_05.csv\", \"TEST_06.csv\", \"TEST_07.csv\", \"TEST_08.csv\", \"TEST_09.csv\". Ensure that all engineered features are added as new columns to the respective DataFrames loaded from these files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ce20e96"
      },
      "source": [
        "## Identify test files\n",
        "\n",
        "### Subtask:\n",
        "Create a list of all the test file paths (`TEST_01.csv` to `TEST_09.csv`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cc0d3d2"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a list containing the file paths for the test datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 718,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bb8d22d",
        "outputId": "c5f610d1-9fbb-4f5c-adc8-5c857c3cf8a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['TEST_00.csv', 'TEST_01.csv', 'TEST_02.csv', 'TEST_03.csv', 'TEST_04.csv', 'TEST_05.csv', 'TEST_06.csv', 'TEST_07.csv', 'TEST_08.csv', 'TEST_09.csv']\n"
          ]
        }
      ],
      "source": [
        "test_files = [f\"TEST_{i:02d}.csv\" for i in range(0, 10)] # Changed range from 1 to 0 to include TEST_00\n",
        "print(test_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d7ad444"
      },
      "source": [
        "## Define feature engineering function\n",
        "\n",
        "### Subtask:\n",
        "Create a function that takes a DataFrame (like the one loaded from a test file) and applies all the necessary feature engineering steps (date features, holiday features, spike/drop, seasonal index, banquet type, etc.) to it, returning the processed DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ec8cbe"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function `engineer_features` that takes a DataFrame and applies all the feature engineering steps. This function will include date features, holiday features, spike/drop detection, weekday/weekend price flags, seasonal index, brunch/hallroom flags, and banquet type merging.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 722,
      "metadata": {
        "id": "b9f8adbb"
      },
      "outputs": [],
      "source": [
        "def engineer_features(df_test, holiday_df, banquet_df, terms_df=None, terms_csv_path=None):\n",
        "    \"\"\"Applies feature engineering steps to a test DataFrame.\n",
        "       terms_df: (선택) 절기 테이블 DataFrame (cols: ['locdate','solar_term'])\n",
        "       terms_csv_path: (선택) terms_df가 없을 때 읽어올 CSV 경로\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    df_test[['영업장명', '메뉴명']] = df_test['영업장명_메뉴명'].str.split('_', expand=True)\n",
        "\n",
        "\n",
        "    # --- 기본 날짜 피처 ---\n",
        "    df_test = df_test.copy()\n",
        "    df_test['영업일자'] = pd.to_datetime(df_test['영업일자'], errors='coerce')\n",
        "    df_test['time_idx'] = (df_test['영업일자'] - df_test['영업일자'].min()).dt.days\n",
        "    df_test['year']    = df_test['영업일자'].dt.year.astype(int)\n",
        "    df_test['month']   = df_test['영업일자'].dt.month.astype(int)\n",
        "    df_test['day']     = df_test['영업일자'].dt.day.astype(int)\n",
        "    df_test['weekday'] = df_test['영업일자'].dt.weekday.astype(int)\n",
        "\n",
        "    # --- season 코드 (이미 사용 중) ---\n",
        "    def get_season_code(m):\n",
        "        if m in (12,1,2): return 0\n",
        "        if m in (3,4,5):  return 1\n",
        "        if m in (6,7,8):  return 2\n",
        "        return 3\n",
        "    df_test['season'] = df_test['month'].apply(get_season_code)\n",
        "\n",
        "    # --- 시즌 피처 생성 (season, season_year, day_of_season) ---\n",
        "    df_test_s = add_season_feats(df_test, '영업일자')\n",
        "\n",
        "    # --- 1차 매핑: (매장메뉴, day_of_season) 시즌 참조 ---\n",
        "    #  * train 쪽에서 만든 curve_ref_season, global_md_dos, global_md_month_day를 사용\n",
        "    #  * 이름 충돌 방지 위해 그대로 사용(위 train 코드 그대로 실행되어 있다고 가정)\n",
        "    df_test_s = df_test_s.merge(\n",
        "        curve_ref_season, on=['영업장명_메뉴명','day_of_season'],\n",
        "        how='left', validate='many_to_one'\n",
        "    ).merge(\n",
        "        global_md_dos, on='day_of_season',\n",
        "        how='left', validate='many_to_one'\n",
        "    ).merge(\n",
        "        global_md_month_day, on=['month','day'],\n",
        "        how='left', validate='many_to_one'\n",
        "    )\n",
        "\n",
        "    # --- 최종 합성: dos → dos_global → (month,day) → 0.5 ---\n",
        "    df_test_s['cum_share_ref_season'] = (\n",
        "        df_test_s['cum_share_ref_dos']\n",
        "                .fillna(df_test_s['cum_share_dos_md'])\n",
        "                .fillna(df_test_s['cum_share_global_md'])\n",
        "                .fillna(0.5)\n",
        "    ).astype('float32')\n",
        "\n",
        "    # --- (권장) 시즌 내 단조 보정 ---\n",
        "    df_test_s = df_test_s.sort_values(['영업장명_메뉴명','season_year','season','day_of_season'])\n",
        "    df_test_s['cum_share_ref_season'] = (\n",
        "        df_test_s.groupby(['영업장명_메뉴명','season_year','season'])['cum_share_ref_season']\n",
        "                .apply(lambda s: s.ffill().bfill().cummax().clip(0,1))\n",
        "                .reset_index(level=[0,1,2], drop=True)\n",
        "    )\n",
        "\n",
        "    # --- 원래 df_test에 결과만 붙이기(원본 순서 보존) ---\n",
        "    df_test['cum_share_ref_season'] = df_test_s.sort_index()['cum_share_ref_season'].values\n",
        "\n",
        "    # =========================\n",
        "    # SOLAR TERM 매핑 시작\n",
        "    # =========================\n",
        "    # 0) terms 준비\n",
        "    if terms_df is None:\n",
        "        if not terms_csv_path:\n",
        "            raise ValueError(\"terms_df 또는 terms_csv_path를 제공해야 solar_term을 생성할 수 있습니다.\")\n",
        "        terms = pd.read_csv(terms_csv_path)\n",
        "    else:\n",
        "        terms = terms_df.copy()\n",
        "\n",
        "    # 1) 절기 구간 정의\n",
        "    terms['locdate'] = pd.to_datetime(terms['locdate'], errors='coerce')\n",
        "    terms_st = terms.sort_values('locdate').reset_index(drop=True)\n",
        "    terms_st['end_date'] = terms_st['locdate'].shift(-1) - pd.Timedelta(days=1)\n",
        "    terms_st.loc[terms_st.index[-1], 'end_date'] = df_test['영업일자'].max()\n",
        "\n",
        "    # 2) asof 머지 (왼쪽 solar_term 삭제해 충돌 방지)\n",
        "    left  = df_test.drop(columns=['solar_term'], errors='ignore').sort_values('영업일자')\n",
        "    right = terms_st[['locdate','end_date','solar_term']].sort_values('locdate')\n",
        "    merged = pd.merge_asof(left, right, left_on='영업일자', right_on='locdate', direction='backward')\n",
        "\n",
        "    # 3) 구간 내만 유효\n",
        "    in_range = merged['영업일자'] <= merged['end_date']\n",
        "    merged['solar_term'] = merged['solar_term'].where(in_range)\n",
        "\n",
        "    # 4) 사이클/경과일\n",
        "    df_test_st = merged.rename(columns={'locdate':'term_start','end_date':'term_end'}).copy()\n",
        "    df_test_st['term_start']  = pd.to_datetime(df_test_st['term_start'])\n",
        "    df_test_st['term_end']    = pd.to_datetime(df_test_st['term_end'])\n",
        "    df_test_st['day_of_term'] = (df_test_st['영업일자'] - df_test_st['term_start']).dt.days.astype('Int16')\n",
        "    df_test_st['term_cycle_id'] = df_test_st['term_start']\n",
        "\n",
        "    # 5) 백업 키\n",
        "    df_test_st['month'] = df_test_st['영업일자'].dt.month.astype('Int8')\n",
        "    df_test_st['day']   = df_test_st['영업일자'].dt.day.astype('Int8')\n",
        "\n",
        "    # 6) 참조 매핑 (train에서 만든 테이블 사용 가정)\n",
        "    # curve_ref_term: ['영업장명_메뉴명','solar_term','day_of_term','cum_share_ref_dot']\n",
        "    # global_md_dot:  ['solar_term','day_of_term','cum_share_term_dot_md']\n",
        "    # global_md_month_day_term: ['month','day','cum_share_global_md_term']\n",
        "    df_test_st = df_test_st.merge(\n",
        "        curve_ref_term, on=['영업장명_메뉴명','solar_term','day_of_term'],\n",
        "        how='left', validate='many_to_one'\n",
        "    ).merge(\n",
        "        global_md_dot, on=['solar_term','day_of_term'],\n",
        "        how='left', validate='many_to_one'\n",
        "    ).merge(\n",
        "        global_md_month_day_term, on=['month','day'],\n",
        "        how='left', validate='many_to_one'\n",
        "    )\n",
        "\n",
        "    df_test_st['cum_share_ref_solar'] = (\n",
        "        df_test_st['cum_share_ref_dot']\n",
        "            .fillna(df_test_st['cum_share_term_dot_md'])\n",
        "            .fillna(df_test_st['cum_share_global_md_term'])\n",
        "            .fillna(0.5)\n",
        "    ).astype('float32')\n",
        "\n",
        "    # 7) 절기 내 단조 보정\n",
        "    df_test_st = df_test_st.sort_values(['영업장명_메뉴명','solar_term','term_cycle_id','day_of_term'])\n",
        "    df_test_st['cum_share_ref_solar'] = (\n",
        "        df_test_st.groupby(['영업장명_메뉴명','solar_term','term_cycle_id'])['cum_share_ref_solar']\n",
        "                  .apply(lambda s: s.ffill().bfill().cummax().clip(0,1))\n",
        "                  .reset_index(level=[0,1,2], drop=True)\n",
        "    )\n",
        "\n",
        "    # 8) 최종 부착(+ solar_term 컬럼도 함께 복원) & 정리\n",
        "    df_test['cum_share_ref_solar'] = df_test_st.sort_index()['cum_share_ref_solar'].values\n",
        "    # ★ 여기서 solar_term을 df_test로 되돌려 붙여줌 (없으면 KeyError 방지)\n",
        "    df_test['solar_term'] = df_test_st.sort_index()['solar_term'].values\n",
        "    # 원하면 nullable 정수로 캐스팅\n",
        "    df_test['solar_term'] = df_test['solar_term'].astype('Int64')\n",
        "\n",
        "    # 보조 컬럼 정리\n",
        "    df_test = df_test.drop(columns=['term_cycle_id','day_of_term'], errors='ignore')\n",
        "\n",
        "    # 2. Holiday features\n",
        "    holiday_df['locdate'] = pd.to_datetime(holiday_df['locdate'])\n",
        "    df_test = pd.merge(\n",
        "        df_test,\n",
        "        holiday_df[['locdate', 'isHoliday']],\n",
        "        how='left',\n",
        "        left_on='영업일자',\n",
        "        right_on='locdate'\n",
        "    )\n",
        "    df_test['is_holiday'] = df_test['isHoliday'].fillna('N').apply(lambda x: 1 if x == 'Y' else 0)\n",
        "    df_test = df_test.drop(['locdate', 'isHoliday'], axis=1)  # 둘 다 삭제\n",
        "\n",
        "    df_test['is_before_holiday'] = df_test['is_holiday'].shift(-1).fillna(0).astype(int)\n",
        "    df_test['is_after_holiday'] = df_test['is_holiday'].shift(1).fillna(0).astype(int)\n",
        "    df_test['is_weekend'] = df_test['weekday'].apply(lambda x: 1 if x in [5, 6] else 0)\n",
        "    df_test[\"is_sandwich\"] = 0\n",
        "    df_test.loc[\n",
        "        (df_test[\"is_holiday\"] == 0) &\n",
        "        (df_test[\"is_holiday\"].shift(1) == 1) &\n",
        "        (df_test[\"is_holiday\"].shift(-1) == 1),\n",
        "        \"is_sandwich\"\n",
        "    ] = 1\n",
        "\n",
        "    # 3. lookup features\n",
        "\n",
        "    feat_cols = ['영업장명_메뉴명',\n",
        "             'menu_category','avg_sales_all_days','avg_sales_nonzero_days',\n",
        "             'zero_sales_day_ratio','is_sparse_menu','is_drink','is_alcohol',\n",
        "             'is_set_menu','demand_volatility','demand_stability', 'is_popular']\n",
        "\n",
        "    train_feats = (train[feat_cols]\n",
        "               .groupby('영업장명_메뉴명', as_index=False)\n",
        "               .agg('first'))  # 필요시 mean/max로 변경\n",
        "\n",
        "    df_test = df_test.merge(train_feats, on='영업장명_메뉴명',\n",
        "                        how='left', validate='many_to_one')\n",
        "    \n",
        "\n",
        "    # --- 선행: quarter, month/day, day_of_quarter 생성 (있으면 건너뜀) ---\n",
        "    if 'quarter' not in df_test.columns:\n",
        "        df_test['quarter'] = df_test['영업일자'].dt.to_period('Q').astype(str).str[-2:]\n",
        "    df_test['quarter'] = df_test['quarter'].map({'Q1':0,'Q2':1,'Q3':2,'Q4':3}).astype('Int8')\n",
        "\n",
        "    if 'year' not in df_test.columns:\n",
        "        df_test['year'] = df_test['영업일자'].dt.year.astype('Int16')\n",
        "    if 'month' not in df_test.columns:\n",
        "        df_test['month'] = df_test['영업일자'].dt.month.astype('Int8')\n",
        "    if 'day' not in df_test.columns:\n",
        "        df_test['day'] = df_test['영업일자'].dt.day.astype('Int8')\n",
        "\n",
        "    q = df_test['영업일자'].dt.to_period('Q')\n",
        "    q_start = q.dt.start_time\n",
        "    df_test['day_of_quarter'] = (df_test['영업일자'] - q_start).dt.days.astype('Int16')\n",
        "\n",
        "    # --- df_train_subset: (영업장명_메뉴명, month, day)별 첫 값만 ---\n",
        "    train_merge_cols = ['영업장명_메뉴명','month','day','cum_share_ref']\n",
        "    df_train_subset = (\n",
        "        train[train_merge_cols]\n",
        "        .groupby(['영업장명_메뉴명','month','day'], as_index=False)\n",
        "        .first()\n",
        "        .rename(columns={'cum_share_ref':'cum_share_ref_md'})  # ← 이름 분리\n",
        "    )\n",
        "\n",
        "    # 타입 맞추기\n",
        "    df_train_subset['month'] = df_train_subset['month'].astype('Int8')\n",
        "    df_train_subset['day']   = df_train_subset['day'].astype('Int8')\n",
        "\n",
        "    # ---- 수정 ----\n",
        "    cr_q = curve_ref.copy()\n",
        "    cr_q = cr_q.rename(columns={'cum_share_ref': 'cum_share_ref_doq'})\n",
        "    cr_q['day_of_quarter'] = cr_q['day_of_quarter'].astype('Int16')\n",
        "\n",
        "\n",
        "    # --- 1차 매핑: (매장메뉴, day_of_quarter) ---\n",
        "    df_test = df_test.merge(\n",
        "        cr_q,\n",
        "        on=['영업장명_메뉴명','day_of_quarter'],\n",
        "        how='left',\n",
        "        validate='many_to_one'  # 1:N 폭증 방지\n",
        "    )\n",
        "\n",
        "    # --- 2차 매핑(백오프): (매장메뉴, month, day) 첫 값 ---\n",
        "    df_test = df_test.merge(\n",
        "        df_train_subset,\n",
        "        on=['영업장명_메뉴명','month','day'],\n",
        "        how='left',\n",
        "        validate='many_to_one'\n",
        "    )\n",
        "\n",
        "    # --- 최종 합성: doq 우선 → md 백오프 → 기본값 0.5 ---\n",
        "    df_test['cum_share_ref'] = (\n",
        "        df_test['cum_share_ref_doq']\n",
        "            .fillna(df_test['cum_share_ref_md'])\n",
        "            .fillna(0.5)\n",
        "    ).astype('float32')\n",
        "\n",
        "    # --- (강력 추천) 분기 내 단조 증가 보정 ---\n",
        "    df_test = df_test.sort_values(['영업장명_메뉴명','year','quarter','영업일자'])\n",
        "    df_test['cum_share_ref'] = (\n",
        "        df_test.groupby(['영업장명_메뉴명','year','quarter'])['cum_share_ref']\n",
        "            .apply(lambda s: s.ffill().bfill().cummax().clip(0, 1))\n",
        "            .reset_index(level=[0,1,2], drop=True)\n",
        "    )\n",
        "\n",
        "    # 임시 컬럼 정리(원하면 남겨도 됨)\n",
        "    df_test = df_test.drop(columns=['cum_share_ref_doq','cum_share_ref_md'], errors='ignore')\n",
        "\n",
        "    # 6. Brunch/Hallroom flags\n",
        "    TARGET = '미라시아_(단체)브런치주중 36,000'\n",
        "    HALL_ROOMS = {'Grand Ballroom', 'Convention Hall', 'Conference L', 'Conference M', 'OPUS'}\n",
        "\n",
        "    tokens = df_test['영업장명_메뉴명'].str.split('_', n=1, expand=True)\n",
        "    store0 = tokens[0].astype(str).str.strip()\n",
        "    store1 = tokens[1].astype(str).str.strip()\n",
        "\n",
        "    # Recalculate hall_dates and brunch_dates from the *original train* data (assuming 'train' df is available globally)\n",
        "    if 'train' in globals():\n",
        "        train_tokens = train['영업장명_메뉴명'].str.split('_', n=1, expand=True)\n",
        "        train_store0 = train_tokens[0].astype(str).str.strip()\n",
        "        train_store1 = train_tokens[1].astype(str).str.strip()\n",
        "\n",
        "        train_hall_mask = (train_store0.eq('연회장')) & (train_store1.isin(HALL_ROOMS)) & (train['매출수량'] > 0)\n",
        "        hall_dates = train.loc[train_hall_mask, '영업일자'].unique()\n",
        "\n",
        "        train_brunch_mask = train['영업장명_메뉴명'].eq(TARGET) & (train['매출수량'] > 0)\n",
        "        brunch_dates = train.loc[train_brunch_mask, '영업일자'].unique()\n",
        "    else:\n",
        "        # Fallback or error handling if train data is not available\n",
        "        print(\"Warning: 'train' DataFrame not found. Cannot calculate brunch_dates and hall_dates.\")\n",
        "        hall_dates = []\n",
        "        brunch_dates = []\n",
        "\n",
        "\n",
        "    if 'brunch_flag' not in df_test.columns:\n",
        "        df_test['brunch_flag'] = 0\n",
        "    if 'hallroom_flag' not in df_test.columns:\n",
        "        df_test['hallroom_flag'] = 0\n",
        "\n",
        "    test_hall_mask = (store0.eq('연회장')) & (store1.isin(HALL_ROOMS))\n",
        "    test_target_row_mask = df_test['영업장명_메뉴명'].eq(TARGET)\n",
        "\n",
        "    df_test.loc[test_hall_mask & df_test['영업일자'].isin(brunch_dates), 'brunch_flag'] = 1\n",
        "    df_test.loc[test_target_row_mask & df_test['영업일자'].isin(hall_dates), 'hallroom_flag'] = 1\n",
        "\n",
        "\n",
        "    # 7. Banquet type\n",
        "    # Ensure '영업일자' is datetime in banquet_df\n",
        "    banquet_df['영업일자'] = pd.to_datetime(banquet_df['영업일자'], errors='coerce')\n",
        "    df_test = pd.merge(df_test, banquet_df[['영업일자', 'banquet_type']], on='영업일자', how='left')\n",
        "    df_test['banquet_type'] = df_test['banquet_type'].fillna(0).astype(int)\n",
        "\n",
        "\n",
        "    return df_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f564b1ec"
      },
      "source": [
        "## Process each test file\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the list of test file paths. For each file:\n",
        "    - Load the CSV into a DataFrame.\n",
        "    - Apply the feature engineering function to the DataFrame.\n",
        "    - Store the processed DataFrame (e.g., in a dictionary or list).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "792bd738"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the test files, apply the feature engineering function to each, and store the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 723,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "61dded98",
        "outputId": "d67c282e-72dc-47d9-cfe2-48bc64ed64f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/garden/Desktop/lgaimers/Hackaton/DataProcessing/re_data_processed\n",
            "Processing TEST_00.csv...\n",
            "Finished processing TEST_00.csv.\n",
            "Processing TEST_01.csv...\n",
            "Finished processing TEST_01.csv.\n",
            "Processing TEST_02.csv...\n",
            "Finished processing TEST_02.csv.\n",
            "Processing TEST_03.csv...\n",
            "Finished processing TEST_03.csv.\n",
            "Processing TEST_04.csv...\n",
            "Finished processing TEST_04.csv.\n",
            "Processing TEST_05.csv...\n",
            "Finished processing TEST_05.csv.\n",
            "Processing TEST_06.csv...\n",
            "Finished processing TEST_06.csv.\n",
            "Processing TEST_07.csv...\n",
            "Finished processing TEST_07.csv.\n",
            "Processing TEST_08.csv...\n",
            "Finished processing TEST_08.csv.\n",
            "Processing TEST_09.csv...\n",
            "Finished processing TEST_09.csv.\n",
            "\n",
            "Sample of processed TEST_00.csv:\n",
            "['영업일자', '영업장명_메뉴명', '매출수량', '영업장명', '메뉴명', 'time_idx', 'year', 'month', 'day', 'weekday', 'season', 'cum_share_ref_season', 'cum_share_ref_solar', 'solar_term', 'is_holiday', 'is_before_holiday', 'is_after_holiday', 'is_weekend', 'is_sandwich', 'menu_category', 'avg_sales_all_days', 'avg_sales_nonzero_days', 'zero_sales_day_ratio', 'is_sparse_menu', 'is_drink', 'is_alcohol', 'is_set_menu', 'demand_volatility', 'demand_stability', 'is_popular', 'quarter', 'day_of_quarter', 'cum_share_ref', 'brunch_flag', 'hallroom_flag', 'banquet_type']\n"
          ]
        }
      ],
      "source": [
        "processed_test_dfs = {}\n",
        "\n",
        "# Load necessary dataframes outside the loop\n",
        "print(os.getcwd())  # Ensure the current working directory is set correctly\n",
        "terms_df = pd.read_csv('../solar_term_2023_2025.csv')  # Load the terms CSV\n",
        "holiday_df = pd.read_csv('../holidays_2023_2025.csv')\n",
        "banquet_df_full = pd.read_csv('re_train_05.csv') # Load the processed train data\n",
        "\n",
        "# Prepare the banquet_df for merging (only date and type)\n",
        "banquet_df_for_merge = banquet_df_full[['영업일자', 'banquet_type']].drop_duplicates()\n",
        "\n",
        "for file_path in test_files:\n",
        "    print(f\"Processing {file_path}...\")\n",
        "    # Load the test data\n",
        "    df_test = pd.read_csv(f\"../../data/test/{file_path}\")\n",
        "\n",
        "    # Apply feature engineering\n",
        "    # Pass the necessary dataframes to the function\n",
        "    processed_df = engineer_features(df_test, holiday_df, banquet_df_for_merge, terms_df)\n",
        "\n",
        "    # Store the processed DataFrame\n",
        "    processed_test_dfs[file_path] = processed_df\n",
        "    print(f\"Finished processing {file_path}.\")\n",
        "\n",
        "# Display the first few rows of one of the processed dataframes to verify\n",
        "if processed_test_dfs:\n",
        "    first_file = list(processed_test_dfs.keys())[0]\n",
        "    print(f\"\\nSample of processed {first_file}:\")\n",
        "    print(processed_test_dfs[first_file].columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a892901e"
      },
      "source": [
        "## Save processed test data (optional)\n",
        "\n",
        "### Subtask:\n",
        "Save each processed test DataFrame to a new CSV file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c705134"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the processed test dataframes and save each one to a CSV file with an added suffix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 724,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f900cd87",
        "outputId": "b253fe25-21b3-4aa6-e0ca-2b4bdae5d7fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved processed TEST_00.csv to ./re_test_processed_03/TEST_00_processed.csv\n",
            "Saved processed TEST_01.csv to ./re_test_processed_03/TEST_01_processed.csv\n",
            "Saved processed TEST_02.csv to ./re_test_processed_03/TEST_02_processed.csv\n",
            "Saved processed TEST_03.csv to ./re_test_processed_03/TEST_03_processed.csv\n",
            "Saved processed TEST_04.csv to ./re_test_processed_03/TEST_04_processed.csv\n",
            "Saved processed TEST_05.csv to ./re_test_processed_03/TEST_05_processed.csv\n",
            "Saved processed TEST_06.csv to ./re_test_processed_03/TEST_06_processed.csv\n",
            "Saved processed TEST_07.csv to ./re_test_processed_03/TEST_07_processed.csv\n",
            "Saved processed TEST_08.csv to ./re_test_processed_03/TEST_08_processed.csv\n",
            "Saved processed TEST_09.csv to ./re_test_processed_03/TEST_09_processed.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "output_dir = \"./re_test_processed_03/\" # Save in the current directory\n",
        "\n",
        "for filename, df_processed in processed_test_dfs.items():\n",
        "    # Construct output filename by adding '_processed' before the extension\n",
        "    base, ext = os.path.splitext(filename)\n",
        "    output_filename = f\"{base}_processed{ext}\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "    # Save the DataFrame to CSV\n",
        "    df_processed.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(f\"Saved processed {filename} to {output_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
